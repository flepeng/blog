<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <script data-ad-client="ca-pub-2488174175014870" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script> <!-- google ad -->
    <meta name="google-site-verification" content="40lMg4eqLLbXoDcpN3h-cEnfmselbQ8tUzNvuC0IRIs" /><!-- google 站点认证 -->
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="description" content="A hexo theme">
    <meta name="keyword"  content="hexo, hexo-theme-lp">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          spider - Scrapy 爬虫 - Hexo-theme-lp
        
    </title>

    <link rel="canonical" href="https://flepeng.github.io/2021/07/30/lepeng-python-spider-spider-Scrapy-爬虫/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
        
<link rel="stylesheet" href="/css/dusign-light.css">

        
<link rel="stylesheet" href="/css/dusign-common-light.css">

        
<link rel="stylesheet" href="/css/font-awesome.css">

        
<link rel="stylesheet" href="/css/toc.css">

        <!-- background effects end -->
    
    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">

    <!-- photography -->
    
<link rel="stylesheet" href="/css/photography.css">


    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- background effects start -->
    
    <!-- background effects end -->

	<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url('/img/header_img/archive-bg.jpg')
                /*post*/
            
        
    }
    
    #signature{
        background-image: url('/img/signature/dusign.png');
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#Python" title="Python">Python</a>
                            
                        </div>
                        <h1>spider - Scrapy 爬虫</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by Lepeng on
                            2021-07-30
                        </span>

                        
                            <div class="blank_box"></div>
                            <span class="meta">
                                Words <span class="post-count">6.3k</span> and
                                Reading Time <span class="post-count">30</span> Minutes
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
    <div class="waveWrapper">
        <div class="wave wave_before" style="background-image: url('/img/wave-light.png')"></div>
        <div class="wave wave_after" style="background-image: url('/img/wave-light.png')"></div>
    </div>
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Lepeng</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/categories/">Categories</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    
                    
                    
                    <li>
                        <a href="http://flepeng.com" target="_blank">chinese_blog</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h2 id="Scrapy-简介"><a href="#Scrapy-简介" class="headerlink" title="Scrapy 简介"></a>Scrapy 简介</h2><p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 其可以应用在数据挖掘，信息处理或存储历史数据等一系列的程序中。其最初是为了页面抓取 (更确切来说, 网络抓取)所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。</p>
<p>Scrapy 使用了 Twisted异步网络库来处理网络通讯。整体架构大致如下</p>
<p><img src="/img/python/spider01.png" alt=""></p>
<p>Scrapy主要包括了以下组件：</p>
<ul>
<li><p><strong>引擎(Scrapy)</strong><br>用来处理整个系统的数据流处理, 触发事务(框架核心)</p>
</li>
<li><p><strong>调度器(Scheduler)</strong><br>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</p>
</li>
<li><p><strong>下载器(Downloader)</strong><br>用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</p>
</li>
<li><p><strong>爬虫(Spiders)</strong><br>爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</p>
</li>
<li><p><strong>项目管道(Pipeline)</strong><br>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</p>
</li>
<li><p><strong>下载器中间件(Downloader Middlewares)</strong><br>位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。</p>
</li>
<li><p><strong>爬虫中间件(Spider Middlewares)</strong><br>介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。</p>
</li>
<li><p><strong>调度中间件(Scheduler Middewares)</strong><br>介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。</p>
</li>
</ul>
<p>Scrapy运行流程大概如下：</p>
<ol>
<li>引擎从调度器中取出一个链接(URL)用于接下来的抓取</li>
<li>引擎把URL封装成一个请求(Request)传给下载器</li>
<li>下载器把资源下载下来，并封装成应答包(Response)</li>
<li>爬虫解析Response</li>
<li>解析出实体（Item）,则交给实体管道进行进一步的处理</li>
<li>解析出的是链接（URL）,则把URL交给调度器等待抓取</li>
</ol>
<h2 id="一、安装"><a href="#一、安装" class="headerlink" title="一、安装"></a>一、安装</h2><p>Linux<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install scrapy</span><br></pre></td></tr></table></figure></p>
<p>Windows</p>
<ol>
<li>pip3 install wheel</li>
<li>下载twisted <a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted" target="_blank" rel="noopener">http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</a></li>
<li>进入下载目录，执行 pip3 install Twisted‑17.1.0‑cp35‑cp35m‑win_amd64.whl</li>
<li>pip3 install scrapy</li>
<li>下载并安装pywin32：<a href="https://sourceforge.net/projects/pywin32/files/" target="_blank" rel="noopener">https://sourceforge.net/projects/pywin32/files/</a></li>
</ol>
<h2 id="二、基本使用"><a href="#二、基本使用" class="headerlink" title="二、基本使用"></a>二、基本使用</h2><h3 id="1-基本命令"><a href="#1-基本命令" class="headerlink" title="1. 基本命令"></a>1. 基本命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1. scrapy startproject 项目名称</span><br><span class="line">   # 在当前目录中创建一个项目文件（类似于Django）</span><br><span class="line"> </span><br><span class="line">2. scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;</span><br><span class="line">   # 创建爬虫应用</span><br><span class="line">      scrapy gensipider -t basic oldboy oldboy.com</span><br><span class="line">      scrapy gensipider -t xmlfeed autohome autohome.com.cn</span><br><span class="line"> </span><br><span class="line">   PS:</span><br><span class="line">      查看所有命令：scrapy gensipider -l</span><br><span class="line">      查看模板命令：scrapy gensipider -d 模板名称</span><br><span class="line"> </span><br><span class="line">3. scrapy list</span><br><span class="line">   # 展示爬虫应用列表</span><br><span class="line"> </span><br><span class="line">4. scrapy crawl 爬虫应用名称</span><br><span class="line">   # 运行单独爬虫应用，要在项目内运行</span><br></pre></td></tr></table></figure>
<h3 id="2-项目结构以及爬虫应用简介"><a href="#2-项目结构以及爬虫应用简介" class="headerlink" title="2.项目结构以及爬虫应用简介"></a>2.项目结构以及爬虫应用简介</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">project_name/</span><br><span class="line">   scrapy.cfg         <span class="comment"># 项目的主配置信息。（真正爬虫相关的配置信息在settings.py文件中）</span></span><br><span class="line">   project_name/</span><br><span class="line">       __init__.py</span><br><span class="line">       items.py       <span class="comment"># 设置数据存储模板，用于结构化数据，如：Django的Model</span></span><br><span class="line">       pipelines.py   <span class="comment"># 数据处理行为，如：一般结构化的数据持久化</span></span><br><span class="line">       settings.py    <span class="comment"># 配置文件，如：递归的层数、并发数，延迟下载等</span></span><br><span class="line">       spiders/       <span class="comment"># 爬虫目录，如：创建文件，编写爬虫规则</span></span><br><span class="line">           __init__.py</span><br><span class="line">           爬虫<span class="number">1.</span>py</span><br><span class="line">           爬虫<span class="number">2.</span>py</span><br><span class="line">           爬虫<span class="number">3.</span>py</span><br></pre></td></tr></table></figure>
<p><em>注意：一般创建爬虫文件时，以网站域名命名</em></p>
<p>爬虫1.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XiaoHuarSpider</span><span class="params">(scrapy.spiders.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"spidername"</span>                 <span class="comment"># 爬虫名称 *****</span></span><br><span class="line">    allowed_domains = [<span class="string">"spider.com"</span>]    <span class="comment"># 允许的域名</span></span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">"http://www.flepeng.com/"</span>,      <span class="comment"># 起始URL</span></span><br><span class="line">    ]</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 访问起始URL并获取结果后的回调函数</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>关于windows编码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import sys,os</span><br><span class="line">sys.stdout&#x3D;io.TextIOWrapper(sys.stdout.buffer,encoding&#x3D;&#39;gb18030&#39;)</span><br></pre></td></tr></table></figure>
<h3 id="3-小试牛刀"><a href="#3-小试牛刀" class="headerlink" title="3.小试牛刀"></a>3.小试牛刀</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy.selector import HtmlXPathSelector  # 新版的好像已经弃用，使用Selector</span><br><span class="line">from scrapy.http.request import Request</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">class DigSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &quot;dig&quot;    # 爬虫应用的名称，通过命令启动爬虫时，使用此参数</span><br><span class="line">    allowed_domains &#x3D; [&quot;chouti.com&quot;]    # 允许的域名</span><br><span class="line">    start_urls &#x3D; [&#39;http:&#x2F;&#x2F;dig.chouti.com&#x2F;&#39;,]    # 起始URL</span><br><span class="line"> </span><br><span class="line">    has_request_set &#x3D; &#123;&#125;</span><br><span class="line"> </span><br><span class="line">    def parse(self, response):</span><br><span class="line">        # 访问首页之后的 解析函数</span><br><span class="line">        print(response.url)</span><br><span class="line">        hxs &#x3D; HtmlXPathSelector(response)</span><br><span class="line">        page_list &#x3D; hxs.select(&#39;&#x2F;&#x2F;div[@id&#x3D;&quot;dig_lcpage&quot;]&#x2F;&#x2F;a[re:test(@href, &quot;&#x2F;all&#x2F;hot&#x2F;recent&#x2F;\d+&quot;)]&#x2F;@href&#39;).extract()</span><br><span class="line">        for page in page_list:</span><br><span class="line">            page_url &#x3D; &#39;http:&#x2F;&#x2F;dig.chouti.com%s&#39; % page</span><br><span class="line">            key &#x3D; self.md5(page_url)</span><br><span class="line"> </span><br><span class="line">            if key not in self.has_request_set:</span><br><span class="line">                self.has_request_set[key] &#x3D; page_url</span><br><span class="line">                obj &#x3D; Request(url&#x3D;page_url, method&#x3D;&#39;GET&#39;, callback&#x3D;self.parse)  # callback 回调函数</span><br><span class="line">                yield obj</span><br><span class="line"> </span><br><span class="line">    @staticmethod</span><br><span class="line">    def md5(val):</span><br><span class="line">        import hashlib</span><br><span class="line">        ha &#x3D; hashlib.md5()</span><br><span class="line">        ha.update(bytes(val, encoding&#x3D;&#39;utf-8&#39;))</span><br><span class="line">        key &#x3D; ha.hexdigest()</span><br><span class="line">        return key</span><br></pre></td></tr></table></figure>
<p>执行此爬虫文件，则在终端进入项目目录执行如下命令：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl dig --nolog <span class="comment"># nolog 表示不打印日志</span></span><br></pre></td></tr></table></figure>
<p>对于上述代码重要之处在于：</p>
<ul>
<li>Request是一个封装用户请求的类，在回调函数中yield该对象表示继续访问</li>
<li>HtmlXpathSelector用于结构化HTML代码并提供选择器功能</li>
</ul>
<h3 id="4-选择器"><a href="#4-选择器" class="headerlink" title="4. 选择器"></a>4. 选择器</h3><p>xpath的路径表达式：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>表达式</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>nodename</td>
<td>选取此节点的所有子节点。</td>
</tr>
<tr>
<td>/</td>
<td>从根节点选取。</td>
</tr>
<tr>
<td>//</td>
<td>从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。</td>
</tr>
<tr>
<td>.</td>
<td>选取当前节点。</td>
</tr>
<tr>
<td>..</td>
<td>选取当前节点的父节点。</td>
</tr>
<tr>
<td>@</td>
<td>选取属性。</td>
</tr>
</tbody>
</table>
</div>
<p>在下面的表格中，列出了一些路径表达式以及表达式的结果：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>路径表达式</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>bookstore</td>
<td>选取 bookstore 元素的所有子节点。</td>
</tr>
<tr>
<td>/bookstore</td>
<td>选取根元素 bookstore。注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！</td>
</tr>
<tr>
<td>bookstore/book</td>
<td>选取属于 bookstore 的子元素的所有 book 元素。</td>
</tr>
<tr>
<td>//book</td>
<td>选取所有 book 子元素，而不管它们在文档中的位置。</td>
</tr>
<tr>
<td>bookstore//book</td>
<td>选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。</td>
</tr>
<tr>
<td>//@lang</td>
<td>选取名为 lang 的所有属性。</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;env python</span><br><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line">from scrapy.selector import Selector, HtmlXPathSelector # 新版好像已弃，使用Selector，用法和这个一样</span><br><span class="line">from scrapy.http import HtmlResponse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">html &#x3D; &quot;&quot;&quot;&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">    &lt;head lang&#x3D;&quot;en&quot;&gt;</span><br><span class="line">        &lt;meta charset&#x3D;&quot;UTF-8&quot;&gt;</span><br><span class="line">        &lt;title&gt;&lt;&#x2F;title&gt;</span><br><span class="line">    &lt;&#x2F;head&gt;</span><br><span class="line">    &lt;body&gt;</span><br><span class="line">        &lt;ul&gt;</span><br><span class="line">            &lt;li class&#x3D;&quot;item-&quot;&gt;&lt;a id&#x3D;&#39;i1&#39; href&#x3D;&quot;link.html&quot;&gt;first item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">            &lt;li class&#x3D;&quot;item-0&quot;&gt;&lt;a id&#x3D;&#39;i2&#39; href&#x3D;&quot;llink.html&quot;&gt;first item&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">            &lt;li class&#x3D;&quot;item-1&quot;&gt;&lt;a href&#x3D;&quot;llink2.html&quot;&gt;second item&lt;span&gt;vv&lt;&#x2F;span&gt;&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;</span><br><span class="line">        &lt;&#x2F;ul&gt;</span><br><span class="line">        &lt;div&gt;&lt;a href&#x3D;&quot;llink2.html&quot;&gt;second item&lt;&#x2F;a&gt;&lt;&#x2F;div&gt;</span><br><span class="line">    &lt;&#x2F;body&gt;</span><br><span class="line">&lt;&#x2F;html&gt;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"> </span><br><span class="line">response &#x3D; HtmlResponse(url&#x3D;&#39;http:&#x2F;&#x2F;example.com&#39;, body&#x3D;html,encoding&#x3D;&#39;utf-8&#39;)</span><br><span class="line"># hxs &#x3D; HtmlXPathSelector(response)</span><br><span class="line"># print(hxs)</span><br><span class="line"># hxs &#x3D; Selector(response&#x3D;response).xpath(&#39;&#x2F;&#x2F;a&#39;)  # 从根目录下查找所有 a 元素</span><br><span class="line"># print(hxs)</span><br><span class="line"># hxs &#x3D; Selector(response&#x3D;response).xpath(&#39;&#x2F;&#x2F;a[2]&#39;)</span><br><span class="line"># print(hxs)</span><br><span class="line"># hxs &#x3D; Selector(response&#x3D;response).xpath(&#39;&#x2F;&#x2F;a[@id]&#39;)</span><br><span class="line"># print(hxs)</span><br><span class="line"># hxs &#x3D; Selector(response&#x3D;response).xpath(&#39;&#x2F;&#x2F;a[@id&#x3D;&quot;i1&quot;]&#39;)</span><br><span class="line"># print(hxs)</span><br><span class="line"># hxs &#x3D; Selector(response&#x3D;response).xpath(&#39;&#x2F;&#x2F;a[@href&#x3D;&quot;link.html&quot;][@id&#x3D;&quot;i1&quot;]&#39;)</span><br><span class="line"># print(hxs)</span><br><span class="line"># hxs &#x3D; Selector(response&#x3D;response).xpath(&#39;&#x2F;&#x2F;a[contains(@href, &quot;link&quot;)]&#39;)</span><br><span class="line"># print(hxs)</span><br><span class="line"># hxs &#x3D; Selector(response&#x3D;response).xpath(&#39;&#x2F;&#x2F;a[starts-with(@href, &quot;link&quot;)]&#39;)</span><br><span class="line"># print(hxs)</span><br><span class="line"># hxs &#x3D; Selector(response&#x3D;response).xpath(&#39;&#x2F;&#x2F;a[re:test(@id, &quot;i\d+&quot;)]&#39;)</span><br><span class="line"># print(hxs)</span><br><span class="line"># hxs &#x3D; Selector(response&#x3D;response).xpath(&#39;&#x2F;&#x2F;a[re:test(@id, &quot;i\d+&quot;)]&#x2F;text()&#39;).extract()</span><br><span class="line"># print(hxs)</span><br><span class="line"># hxs &#x3D; Selector(response&#x3D;response).xpath(&#39;&#x2F;&#x2F;a[re:test(@id, &quot;i\d+&quot;)]&#x2F;@href&#39;).extract()</span><br><span class="line"># print(hxs)</span><br><span class="line"># hxs &#x3D; Selector(response&#x3D;response).xpath(&#39;&#x2F;html&#x2F;body&#x2F;ul&#x2F;li&#x2F;a&#x2F;@href&#39;).extract()</span><br><span class="line"># print(hxs)</span><br><span class="line"># hxs &#x3D; Selector(response&#x3D;response).xpath(&#39;&#x2F;&#x2F;body&#x2F;ul&#x2F;li&#x2F;a&#x2F;@href&#39;).extract_first()</span><br><span class="line"># print(hxs)</span><br><span class="line"># ul_list &#x3D; Selector(response&#x3D;response).xpath(&#39;&#x2F;&#x2F;body&#x2F;ul&#x2F;li&#39;)</span><br><span class="line"># for item in ul_list:</span><br><span class="line">#     v &#x3D; item.xpath(&#39;.&#x2F;a&#x2F;span&#39;)</span><br><span class="line">#     # 或</span><br><span class="line">#     # v &#x3D; item.xpath(&#39;a&#x2F;span&#39;)</span><br><span class="line">#     # 或</span><br><span class="line">#     # v &#x3D; item.xpath(&#39;*&#x2F;a&#x2F;span&#39;)</span><br><span class="line">#     print(v)</span><br></pre></td></tr></table></figure>
<p>示例：自动登陆抽屉并点赞</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import scrapy</span><br><span class="line">from scrapy.selector import HtmlXPathSelector</span><br><span class="line">from scrapy.http.request import Request</span><br><span class="line">from scrapy.http.cookies import CookieJar</span><br><span class="line">from scrapy import FormRequest</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">class ChouTiSpider(scrapy.Spider):</span><br><span class="line">    # 爬虫应用的名称，通过此名称启动爬虫命令</span><br><span class="line">    name &#x3D; &quot;chouti&quot;</span><br><span class="line">    # 允许的域名</span><br><span class="line">    allowed_domains &#x3D; [&quot;chouti.com&quot;]</span><br><span class="line"> </span><br><span class="line">    cookie_dict &#x3D; &#123;&#125;</span><br><span class="line">    has_request_set &#x3D; &#123;&#125;</span><br><span class="line"> </span><br><span class="line">    def start_requests(self):</span><br><span class="line">        url &#x3D; &#39;http:&#x2F;&#x2F;dig.chouti.com&#x2F;&#39;</span><br><span class="line">        # return [Request(url&#x3D;url, callback&#x3D;self.login)]</span><br><span class="line">        yield Request(url&#x3D;url, callback&#x3D;self.login)</span><br><span class="line"> </span><br><span class="line">    def login(self, response):</span><br><span class="line">        cookie_jar &#x3D; CookieJar()</span><br><span class="line">        cookie_jar.extract_cookies(response, response.request)</span><br><span class="line">        for k, v in cookie_jar._cookies.items():</span><br><span class="line">            for i, j in v.items():</span><br><span class="line">                for m, n in j.items():</span><br><span class="line">                    self.cookie_dict[m] &#x3D; n.value</span><br><span class="line"> </span><br><span class="line">        req &#x3D; Request(</span><br><span class="line">            url&#x3D;&#39;http:&#x2F;&#x2F;dig.chouti.com&#x2F;login&#39;,</span><br><span class="line">            method&#x3D;&#39;POST&#39;,</span><br><span class="line">            headers&#x3D;&#123;&#39;Content-Type&#39;: &#39;application&#x2F;x-www-form-urlencoded; charset&#x3D;UTF-8&#39;&#125;,</span><br><span class="line">            body&#x3D;&#39;phone&#x3D;8615131255089&amp;password&#x3D;pppppppp&amp;oneMonth&#x3D;1&#39;,</span><br><span class="line">            cookies&#x3D;self.cookie_dict,</span><br><span class="line">            callback&#x3D;self.check_login</span><br><span class="line">        )</span><br><span class="line">        yield req</span><br><span class="line"> </span><br><span class="line">    def check_login(self, response):</span><br><span class="line">        req &#x3D; Request(</span><br><span class="line">            url&#x3D;&#39;http:&#x2F;&#x2F;dig.chouti.com&#x2F;&#39;,</span><br><span class="line">            method&#x3D;&#39;GET&#39;,</span><br><span class="line">            callback&#x3D;self.show,</span><br><span class="line">            cookies&#x3D;self.cookie_dict,</span><br><span class="line">            dont_filter&#x3D;True</span><br><span class="line">        )</span><br><span class="line">        yield req</span><br><span class="line"> </span><br><span class="line">    def show(self, response):</span><br><span class="line">        # print(response)</span><br><span class="line">        hxs &#x3D; HtmlXPathSelector(response)</span><br><span class="line">        news_list &#x3D; hxs.select(&#39;&#x2F;&#x2F;div[@id&#x3D;&quot;content-list&quot;]&#x2F;div[@class&#x3D;&quot;item&quot;]&#39;)</span><br><span class="line">        for new in news_list:</span><br><span class="line">            # temp &#x3D; new.xpath(&#39;div&#x2F;div[@class&#x3D;&quot;part2&quot;]&#x2F;@share-linkid&#39;).extract()</span><br><span class="line">            link_id &#x3D; new.xpath(&#39;*&#x2F;div[@class&#x3D;&quot;part2&quot;]&#x2F;@share-linkid&#39;).extract_first()</span><br><span class="line">            yield Request(</span><br><span class="line">                url&#x3D;&#39;http:&#x2F;&#x2F;dig.chouti.com&#x2F;link&#x2F;vote?linksId&#x3D;%s&#39; %(link_id,),</span><br><span class="line">                method&#x3D;&#39;POST&#39;,</span><br><span class="line">                cookies&#x3D;self.cookie_dict,</span><br><span class="line">                callback&#x3D;self.do_favor</span><br><span class="line">            )</span><br><span class="line"> </span><br><span class="line">        page_list &#x3D; hxs.select(&#39;&#x2F;&#x2F;div[@id&#x3D;&quot;dig_lcpage&quot;]&#x2F;&#x2F;a[re:test(@href, &quot;&#x2F;all&#x2F;hot&#x2F;recent&#x2F;\d+&quot;)]&#x2F;@href&#39;).extract()</span><br><span class="line">        for page in page_list:</span><br><span class="line"> </span><br><span class="line">            page_url &#x3D; &#39;http:&#x2F;&#x2F;dig.chouti.com%s&#39; % page</span><br><span class="line">            import hashlib</span><br><span class="line">            hash &#x3D; hashlib.md5()</span><br><span class="line">            hash.update(bytes(page_url,encoding&#x3D;&#39;utf-8&#39;))</span><br><span class="line">            key &#x3D; hash.hexdigest()</span><br><span class="line">            if key in self.has_request_set:</span><br><span class="line">                pass</span><br><span class="line">            else:</span><br><span class="line">                self.has_request_set[key] &#x3D; page_url</span><br><span class="line">                yield Request(</span><br><span class="line">                    url&#x3D;page_url,</span><br><span class="line">                    method&#x3D;&#39;GET&#39;,</span><br><span class="line">                    callback&#x3D;self.show</span><br><span class="line">                )</span><br><span class="line"> </span><br><span class="line">    def do_favor(self, response):</span><br><span class="line">        print(response.text)</span><br></pre></td></tr></table></figure>
<p>处理Cookie</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.http.response.html <span class="keyword">import</span> HtmlResponse</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">from</span> scrapy.http.cookies <span class="keyword">import</span> CookieJar</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChoutiSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"chouti"</span></span><br><span class="line">    allowed_domains = [<span class="string">"chouti.com"</span>]</span><br><span class="line">    start_urls = (<span class="string">'http://www.chouti.com/'</span>,)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        url = <span class="string">'http://dig.chouti.com/'</span></span><br><span class="line">        <span class="keyword">yield</span> Request(url=url, callback=self.login, meta=&#123;<span class="string">'cookiejar'</span>: <span class="literal">True</span>&#125;)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        print(response.headers.getlist(<span class="string">'Set-Cookie'</span>))</span><br><span class="line">        req = Request(</span><br><span class="line">            url=<span class="string">'http://dig.chouti.com/login'</span>,</span><br><span class="line">            method=<span class="string">'POST'</span>,</span><br><span class="line">            headers=&#123;<span class="string">'Content-Type'</span>: <span class="string">'application/x-www-form-urlencoded; charset=UTF-8'</span>&#125;,</span><br><span class="line">            body=<span class="string">'phone=8613121758648&amp;password=woshiniba&amp;oneMonth=1'</span>,</span><br><span class="line">            callback=self.check_login,</span><br><span class="line">            meta=&#123;<span class="string">'cookiejar'</span>: <span class="literal">True</span>&#125;</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">yield</span> req</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        print(response.text)</span><br></pre></td></tr></table></figure>
<p><em>注意：settings.py中设置DEPTH_LIMIT = 1来指定“递归”的层数。</em></p>
<h3 id="5-格式化处理-pipelines"><a href="#5-格式化处理-pipelines" class="headerlink" title="5. 格式化处理 pipelines"></a>5. 格式化处理 pipelines</h3><p>上述实例只是简单的处理，所以在parse方法中直接处理。如果对于想要获取更多的数据处理，则可以利用Scrapy的items将数据格式化，然后统一交由pipelines来处理。</p>
<p>spiders/xiahuar.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy.selector import HtmlXPathSelector</span><br><span class="line">from scrapy.http.request import Request</span><br><span class="line">from scrapy.http.cookies import CookieJar</span><br><span class="line">from scrapy import FormRequest</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">class XiaoHuarSpider(scrapy.Spider):</span><br><span class="line">    name &#x3D; &quot;xiaohuar&quot;</span><br><span class="line">    allowed_domains &#x3D; [&quot;xiaohuar.com&quot;]</span><br><span class="line"> </span><br><span class="line">    start_urls &#x3D; [&quot;http:&#x2F;&#x2F;www.xiaohuar.com&#x2F;list-1-1.html&quot;,]</span><br><span class="line">    # setting 中的配置pipelines</span><br><span class="line">    # custom_settings &#x3D; &#123;</span><br><span class="line">    #     &#39;ITEM_PIPELINES&#39;:&#123;</span><br><span class="line">    #         &#39;spider1.pipelines.JsonPipeline&#39;: 100</span><br><span class="line">    #     &#125;</span><br><span class="line">    # &#125;</span><br><span class="line">    has_request_set &#x3D; &#123;&#125;</span><br><span class="line"> </span><br><span class="line">    def parse(self, response):</span><br><span class="line">        # 分析页面</span><br><span class="line">        # 找到页面中符合规则的内容（校花图片），保存</span><br><span class="line">        # 找到所有的a标签，再访问其他a标签，一层一层的搞下去</span><br><span class="line"> </span><br><span class="line">        hxs &#x3D; HtmlXPathSelector(response)</span><br><span class="line"> </span><br><span class="line">        items &#x3D; hxs.select(&#39;&#x2F;&#x2F;div[@class&#x3D;&quot;item_list infinite_scroll&quot;]&#x2F;div&#39;)</span><br><span class="line">        for item in items:</span><br><span class="line">            src &#x3D; item.select(&#39;.&#x2F;&#x2F;div[@class&#x3D;&quot;img&quot;]&#x2F;a&#x2F;img&#x2F;@src&#39;).extract_first()</span><br><span class="line">            name &#x3D; item.select(&#39;.&#x2F;&#x2F;div[@class&#x3D;&quot;img&quot;]&#x2F;span&#x2F;text()&#39;).extract_first()</span><br><span class="line">            school &#x3D; item.select(&#39;.&#x2F;&#x2F;div[@class&#x3D;&quot;img&quot;]&#x2F;div[@class&#x3D;&quot;btns&quot;]&#x2F;a&#x2F;text()&#39;).extract_first()</span><br><span class="line">            url &#x3D; &quot;http:&#x2F;&#x2F;www.xiaohuar.com%s&quot; % src</span><br><span class="line">            from ..items import XiaoHuarItem</span><br><span class="line">            obj &#x3D; XiaoHuarItem(name&#x3D;name, school&#x3D;school, url&#x3D;url)</span><br><span class="line">            yield obj</span><br><span class="line"> </span><br><span class="line">        urls &#x3D; hxs.select(&#39;&#x2F;&#x2F;a[re:test(@href, &quot;http:&#x2F;&#x2F;www.xiaohuar.com&#x2F;list-1-\d+.html&quot;)]&#x2F;@href&#39;)</span><br><span class="line">        for url in urls:</span><br><span class="line">            key &#x3D; self.md5(url)</span><br><span class="line">            if key in self.has_request_set:</span><br><span class="line">                pass</span><br><span class="line">            else:</span><br><span class="line">                self.has_request_set[key] &#x3D; url</span><br><span class="line">                req &#x3D; Request(url&#x3D;url,method&#x3D;&#39;GET&#39;,callback&#x3D;self.parse)</span><br><span class="line">                yield req</span><br><span class="line"> </span><br><span class="line">    @staticmethod</span><br><span class="line">    def md5(val):</span><br><span class="line">        import hashlib</span><br><span class="line">        ha &#x3D; hashlib.md5()</span><br><span class="line">        ha.update(bytes(val, encoding&#x3D;&#39;utf-8&#39;))</span><br><span class="line">        key &#x3D; ha.hexdigest()</span><br><span class="line">        return key</span><br></pre></td></tr></table></figure>
<p>items</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">XiaoHuarItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    school = scrapy.Field()</span><br><span class="line">    url = scrapy.Field()</span><br></pre></td></tr></table></figure>
<p>pipelines</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.file = open(<span class="string">'xiaohua.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        v = json.dumps(dict(item), ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">        self.file.write(v)</span><br><span class="line">        self.file.write(<span class="string">'\n'</span>)</span><br><span class="line">        self.file.flush()</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FilePipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'imgs'</span>):</span><br><span class="line">            os.makedirs(<span class="string">'imgs'</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        response = requests.get(item[<span class="string">'url'</span>], stream=<span class="literal">True</span>)</span><br><span class="line">        file_name = <span class="string">'%s_%s.jpg'</span> % (item[<span class="string">'name'</span>], item[<span class="string">'school'</span>])</span><br><span class="line">        <span class="keyword">with</span> open(os.path.join(<span class="string">'imgs'</span>, file_name), mode=<span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(response.content)</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p>settings</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'spider1.pipelines.JsonPipeline'</span>: <span class="number">100</span>,</span><br><span class="line">   <span class="string">'spider1.pipelines.FilePipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 后面的整数值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。</span></span><br></pre></td></tr></table></figure>
<p>对于pipeline可以做更多，如下：</p>
<p>自定义pipeline格式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,v)</span>:</span></span><br><span class="line">        self.value = v</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="comment"># 运行pipeline时会调用此函数，操作并进行持久化</span></span><br><span class="line">        <span class="comment"># return表示会被后续的pipeline继续处理</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 表示将item丢弃，不会被后续pipeline处理</span></span><br><span class="line">        <span class="comment"># raise DropItem()</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="comment"># 初始化时候，用于创建pipeline对象</span></span><br><span class="line">        val = crawler.settings.getint(<span class="string">'MMMM'</span>)</span><br><span class="line">        <span class="keyword">return</span> cls(val)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        <span class="comment"># 爬虫开始执行时，调用</span></span><br><span class="line">        print(<span class="string">'000000'</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        <span class="comment"># 爬虫关闭时，被调用</span></span><br><span class="line">        print(<span class="string">'111111'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="6-中间件"><a href="#6-中间件" class="headerlink" title="6.中间件"></a>6.中间件</h3><h4 id="爬虫中间件"><a href="#爬虫中间件" class="headerlink" title="爬虫中间件"></a>爬虫中间件</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpiderMiddleware</span><span class="params">(object)</span>:</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_spider_input</span><span class="params">(self,response, spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        下载完成，执行，然后交给parse处理</span></span><br><span class="line"><span class="string">        :param response: </span></span><br><span class="line"><span class="string">        :param spider: </span></span><br><span class="line"><span class="string">        :return: </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_spider_output</span><span class="params">(self,response, result, spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        spider处理完成，返回时调用</span></span><br><span class="line"><span class="string">        :param response:</span></span><br><span class="line"><span class="string">        :param result:</span></span><br><span class="line"><span class="string">        :param spider:</span></span><br><span class="line"><span class="string">        :return: 必须返回包含 Request 或 Item 对象的可迭代对象(iterable)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_spider_exception</span><span class="params">(self,response, exception, spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        异常调用</span></span><br><span class="line"><span class="string">        :param response:</span></span><br><span class="line"><span class="string">        :param exception:</span></span><br><span class="line"><span class="string">        :param spider:</span></span><br><span class="line"><span class="string">        :return: None,继续交给后续中间件处理异常；含 Response 或 Item 的可迭代对象(iterable)，交给调度器或pipeline</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_start_requests</span><span class="params">(self,start_requests, spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        爬虫启动时调用</span></span><br><span class="line"><span class="string">        :param start_requests:</span></span><br><span class="line"><span class="string">        :param spider:</span></span><br><span class="line"><span class="string">        :return: 包含 Request 对象的可迭代对象</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> start_requests</span><br></pre></td></tr></table></figure>
<h4 id="下载器中间件"><a href="#下载器中间件" class="headerlink" title="下载器中间件"></a>下载器中间件</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DownMiddleware1</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        请求需要被下载时，经过所有下载器中间件的process_request调用</span></span><br><span class="line"><span class="string">        :param request: </span></span><br><span class="line"><span class="string">        :param spider: </span></span><br><span class="line"><span class="string">        :return:  </span></span><br><span class="line"><span class="string">            None,继续后续中间件去下载；</span></span><br><span class="line"><span class="string">            Response对象，停止process_request的执行，开始执行process_response</span></span><br><span class="line"><span class="string">            Request对象，停止中间件的执行，将Request重新调度器</span></span><br><span class="line"><span class="string">            raise IgnoreRequest异常，停止process_request的执行，开始执行process_exception</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span><span class="params">(self, request, response, spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        spider处理完成，返回时调用</span></span><br><span class="line"><span class="string">        :param response:</span></span><br><span class="line"><span class="string">        :param result:</span></span><br><span class="line"><span class="string">        :param spider:</span></span><br><span class="line"><span class="string">        :return: </span></span><br><span class="line"><span class="string">            Response 对象：转交给其他中间件process_response</span></span><br><span class="line"><span class="string">            Request 对象：停止中间件，request会被重新调度下载</span></span><br><span class="line"><span class="string">            raise IgnoreRequest 异常：调用Request.errback</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        print(<span class="string">'response1'</span>)</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_exception</span><span class="params">(self, request, exception, spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        当下载处理器(download handler)或 process_request() (下载中间件)抛出异常</span></span><br><span class="line"><span class="string">        :param response:</span></span><br><span class="line"><span class="string">        :param exception:</span></span><br><span class="line"><span class="string">        :param spider:</span></span><br><span class="line"><span class="string">        :return: </span></span><br><span class="line"><span class="string">            None：继续交给后续中间件处理异常；</span></span><br><span class="line"><span class="string">            Response对象：停止后续process_exception方法</span></span><br><span class="line"><span class="string">            Request对象：停止中间件，request将会被重新调用下载</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h3 id="7-自定制命令"><a href="#7-自定制命令" class="headerlink" title="7. 自定制命令"></a>7. 自定制命令</h3><ul>
<li>在spiders同级创建任意目录，如：commands</li>
<li>在其中创建 crawlall.py 文件 （此处文件名就是自定义的命令）</li>
</ul>
<p>crawlall.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.commands <span class="keyword">import</span> ScrapyCommand</span><br><span class="line"><span class="keyword">from</span> scrapy.utils.project <span class="keyword">import</span> get_project_settings</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Command</span><span class="params">(ScrapyCommand)</span>:</span></span><br><span class="line">    requires_project = <span class="literal">True</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">syntax</span><span class="params">(self)</span>:</span>        <span class="comment"># 命令的参数</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'[options]'</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">short_desc</span><span class="params">(self)</span>:</span>    <span class="comment"># 命令的描述</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Runs all of the spiders'</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self, args, opts)</span>:</span></span><br><span class="line">        spider_list = self.crawler_process.spiders.list()</span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> spider_list:</span><br><span class="line">            self.crawler_process.crawl(name, **opts.__dict__)</span><br><span class="line">        self.crawler_process.start()</span><br></pre></td></tr></table></figure>
<ul>
<li>在settings.py 中添加配置 COMMANDS_MODULE = ‘项目名称.目录名称’</li>
<li>在项目目录执行命令：scrapy crawlall </li>
</ul>
<p>单个爬虫</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">from scrapy.cmdline import execute</span><br><span class="line"> </span><br><span class="line">if __name__ &#x3D;&#x3D; &#39;__main__&#39;:</span><br><span class="line">    execute([&quot;scrapy&quot;,&quot;github&quot;,&quot;--nolog&quot;])</span><br></pre></td></tr></table></figure>
<h3 id="8-自定义扩展"><a href="#8-自定义扩展" class="headerlink" title="8. 自定义扩展"></a>8. 自定义扩展</h3><p>自定义扩展时，利用信号在指定位置注册制定操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyExtension</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, value)</span>:</span></span><br><span class="line">        self.value = value</span><br><span class="line"> </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        val = crawler.settings.getint(<span class="string">'MMMM'</span>)</span><br><span class="line">        ext = cls(val)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 注册信号</span></span><br><span class="line">        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)</span><br><span class="line">        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">return</span> ext</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_opened</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        print(<span class="string">'open'</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_closed</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        print(<span class="string">'close'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="9-避免重复访问"><a href="#9-避免重复访问" class="headerlink" title="9. 避免重复访问"></a>9. 避免重复访问</h3><p>scrapy默认使用 scrapy.dupefilter.RFPDupeFilter 进行去重，相关配置有：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DUPEFILTER_CLASS = <span class="string">'scrapy.dupefilter.RFPDupeFilter'</span></span><br><span class="line">DUPEFILTER_DEBUG = <span class="literal">False</span></span><br><span class="line">JOBDIR = <span class="string">"保存范文记录的日志路径，如：/root/"</span>  <span class="comment"># 最终路径为 /root/requests.seen</span></span><br></pre></td></tr></table></figure>
<p>自定义URL去重操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RepeatUrl</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.visited_url = set()</span><br><span class="line"> </span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_settings</span><span class="params">(cls, settings)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化时，调用</span></span><br><span class="line"><span class="string">        :param settings: </span></span><br><span class="line"><span class="string">        :return: </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> cls()</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">request_seen</span><span class="params">(self, request)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        检测当前请求是否已经被访问过</span></span><br><span class="line"><span class="string">        :param request: </span></span><br><span class="line"><span class="string">        :return: True表示已经访问过；False表示未访问过</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> request.url <span class="keyword">in</span> self.visited_url:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        self.visited_url.add(request.url)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        开始爬取请求时，调用</span></span><br><span class="line"><span class="string">        :return: </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        print(<span class="string">'open replication'</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self, reason)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        结束爬虫爬取时，调用</span></span><br><span class="line"><span class="string">        :param reason: </span></span><br><span class="line"><span class="string">        :return: </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        print(<span class="string">'close replication'</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log</span><span class="params">(self, request, spider)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        记录日志</span></span><br><span class="line"><span class="string">        :param request: </span></span><br><span class="line"><span class="string">        :param spider: </span></span><br><span class="line"><span class="string">        :return: </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        print(<span class="string">'repeat'</span>, request.url)</span><br></pre></td></tr></table></figure>
<h3 id="10-其他"><a href="#10-其他" class="headerlink" title="10.其他"></a>10.其他</h3><p>settings</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Scrapy settings for step8_king project</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># For simplicity, this file contains only settings considered important or</span></span><br><span class="line"><span class="comment"># commonly used. You can find more settings consulting the documentation:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     http://doc.scrapy.org/en/latest/topics/settings.html</span></span><br><span class="line"><span class="comment">#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line"><span class="comment">#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 1. 爬虫名称</span></span><br><span class="line">BOT_NAME = <span class="string">'step8_king'</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 2. 爬虫应用路径</span></span><br><span class="line">SPIDER_MODULES = [<span class="string">'step8_king.spiders'</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">'step8_king.spiders'</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span></span><br><span class="line"><span class="comment"># 3. 客户端 user-agent请求头</span></span><br><span class="line"><span class="comment"># USER_AGENT = 'step8_king (+http://www.yourdomain.com)'</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line"><span class="comment"># 4. 禁止爬虫配置，应该开启，看看是否允许</span></span><br><span class="line"><span class="comment"># ROBOTSTXT_OBEY = False</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span></span><br><span class="line"><span class="comment"># 5. 并发请求数</span></span><br><span class="line"><span class="comment"># CONCURRENT_REQUESTS = 4</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Configure a delay for requests for the same website (default: 0)</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay</span></span><br><span class="line"><span class="comment"># See also autothrottle settings and docs</span></span><br><span class="line"><span class="comment"># 6. 延迟下载秒数</span></span><br><span class="line"><span class="comment"># DOWNLOAD_DELAY = 2</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># The download delay setting will honor only one of:</span></span><br><span class="line"><span class="comment"># 7. 单域名访问并发数，并且延迟下次秒数也应用在每个域名</span></span><br><span class="line"><span class="comment"># CONCURRENT_REQUESTS_PER_DOMAIN = 2</span></span><br><span class="line"><span class="comment"># 单IP访问并发数，如果有值则忽略：CONCURRENT_REQUESTS_PER_DOMAIN，并且延迟下次秒数也应用在每个IP</span></span><br><span class="line"><span class="comment"># CONCURRENT_REQUESTS_PER_IP = 3</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Disable cookies (enabled by default)</span></span><br><span class="line"><span class="comment"># 8. 是否支持cookie，cookiejar进行操作cookie</span></span><br><span class="line"><span class="comment"># COOKIES_ENABLED = True</span></span><br><span class="line"><span class="comment"># COOKIES_DEBUG = True</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Disable Telnet Console (enabled by default)</span></span><br><span class="line"><span class="comment"># 9. Telnet用于查看当前爬虫的信息，操作爬虫等...</span></span><br><span class="line"><span class="comment">#    使用telnet ip port ，然后通过命令操作</span></span><br><span class="line"><span class="comment"># TELNETCONSOLE_ENABLED = True</span></span><br><span class="line"><span class="comment"># TELNETCONSOLE_HOST = '127.0.0.1'</span></span><br><span class="line"><span class="comment"># TELNETCONSOLE_PORT = [6023,]</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 10. 默认请求头</span></span><br><span class="line"><span class="comment"># Override the default request headers:</span></span><br><span class="line"><span class="comment"># DEFAULT_REQUEST_HEADERS = &#123;</span></span><br><span class="line"><span class="comment">#     'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',</span></span><br><span class="line"><span class="comment">#     'Accept-Language': 'en',</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># Configure item pipelines</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"><span class="comment"># 11. 定义pipeline处理请求</span></span><br><span class="line"><span class="comment"># ITEM_PIPELINES = &#123;</span></span><br><span class="line"><span class="comment">#    'step8_king.pipelines.JsonPipeline': 700,</span></span><br><span class="line"><span class="comment">#    'step8_king.pipelines.FilePipeline': 500,</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 12. 自定义扩展，基于信号进行调用</span></span><br><span class="line"><span class="comment"># Enable or disable extensions</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/extensions.html</span></span><br><span class="line"><span class="comment"># EXTENSIONS = &#123;</span></span><br><span class="line"><span class="comment">#     # 'step8_king.extensions.MyExtension': 500,</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 13. 爬虫允许的最大深度，可以通过meta查看当前深度；0表示无深度</span></span><br><span class="line"><span class="comment"># DEPTH_LIMIT = 3</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 14. 爬取时，0表示深度优先Lifo(默认)；1表示广度优先FiFo</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 后进先出，深度优先</span></span><br><span class="line"><span class="comment"># DEPTH_PRIORITY = 0</span></span><br><span class="line"><span class="comment"># SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleLifoDiskQueue'</span></span><br><span class="line"><span class="comment"># SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.LifoMemoryQueue'</span></span><br><span class="line"><span class="comment"># 先进先出，广度优先</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># DEPTH_PRIORITY = 1</span></span><br><span class="line"><span class="comment"># SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleFifoDiskQueue'</span></span><br><span class="line"><span class="comment"># SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.FifoMemoryQueue'</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 15. 调度器队列</span></span><br><span class="line"><span class="comment"># SCHEDULER = 'scrapy.core.scheduler.Scheduler'</span></span><br><span class="line"><span class="comment"># from scrapy.core.scheduler import Scheduler</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 16. 访问URL去重</span></span><br><span class="line"><span class="comment"># DUPEFILTER_CLASS = 'step8_king.duplication.RepeatUrl'</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># Enable and configure the AutoThrottle extension (disabled by default)</span></span><br><span class="line"><span class="comment"># See http://doc.scrapy.org/en/latest/topics/autothrottle.html</span></span><br><span class="line"> </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">17. 自动限速算法</span></span><br><span class="line"><span class="string">    from scrapy.contrib.throttle import AutoThrottle</span></span><br><span class="line"><span class="string">    自动限速设置</span></span><br><span class="line"><span class="string">    1. 获取最小延迟 DOWNLOAD_DELAY</span></span><br><span class="line"><span class="string">    2. 获取最大延迟 AUTOTHROTTLE_MAX_DELAY</span></span><br><span class="line"><span class="string">    3. 设置初始下载延迟 AUTOTHROTTLE_START_DELAY</span></span><br><span class="line"><span class="string">    4. 当请求下载完成后，获取其"连接"时间 latency，即：请求连接到接受到响应头之间的时间</span></span><br><span class="line"><span class="string">    5. 用于计算的... AUTOTHROTTLE_TARGET_CONCURRENCY</span></span><br><span class="line"><span class="string">    target_delay = latency / self.target_concurrency</span></span><br><span class="line"><span class="string">    new_delay = (slot.delay + target_delay) / 2.0 # 表示上一次的延迟时间</span></span><br><span class="line"><span class="string">    new_delay = max(target_delay, new_delay)</span></span><br><span class="line"><span class="string">    new_delay = min(max(self.mindelay, new_delay), self.maxdelay)</span></span><br><span class="line"><span class="string">    slot.delay = new_delay</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 开始自动限速</span></span><br><span class="line"><span class="comment"># AUTOTHROTTLE_ENABLED = True</span></span><br><span class="line"><span class="comment"># The initial download delay</span></span><br><span class="line"><span class="comment"># 初始下载延迟</span></span><br><span class="line"><span class="comment"># AUTOTHROTTLE_START_DELAY = 5</span></span><br><span class="line"><span class="comment"># The maximum download delay to be set in case of high latencies</span></span><br><span class="line"><span class="comment"># 最大下载延迟</span></span><br><span class="line"><span class="comment"># AUTOTHROTTLE_MAX_DELAY = 10</span></span><br><span class="line"><span class="comment"># The average number of requests Scrapy should be sending in parallel to each remote server</span></span><br><span class="line"><span class="comment"># 平均每秒并发数</span></span><br><span class="line"><span class="comment"># AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Enable showing throttling stats for every response received:</span></span><br><span class="line"><span class="comment"># 是否显示</span></span><br><span class="line"><span class="comment"># AUTOTHROTTLE_DEBUG = True</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Enable and configure HTTP caching (disabled by default)</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">18. 启用缓存</span></span><br><span class="line"><span class="string">    目的用于将已经发送的请求或相应缓存下来，以便以后使用</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware</span></span><br><span class="line"><span class="string">    from scrapy.extensions.httpcache import DummyPolicy</span></span><br><span class="line"><span class="string">    from scrapy.extensions.httpcache import FilesystemCacheStorage</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 是否启用缓存策略</span></span><br><span class="line"><span class="comment"># HTTPCACHE_ENABLED = True</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 缓存策略：所有请求均缓存，下次在请求直接访问原来的缓存即可</span></span><br><span class="line"><span class="comment"># HTTPCACHE_POLICY = "scrapy.extensions.httpcache.DummyPolicy"</span></span><br><span class="line"><span class="comment"># 缓存策略：根据Http响应头：Cache-Control、Last-Modified 等进行缓存的策略</span></span><br><span class="line"><span class="comment"># HTTPCACHE_POLICY = "scrapy.extensions.httpcache.RFC2616Policy"</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 缓存超时时间</span></span><br><span class="line"><span class="comment"># HTTPCACHE_EXPIRATION_SECS = 0</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 缓存保存路径</span></span><br><span class="line"><span class="comment"># HTTPCACHE_DIR = 'httpcache'</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 缓存忽略的Http状态码</span></span><br><span class="line"><span class="comment"># HTTPCACHE_IGNORE_HTTP_CODES = []</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 缓存存储的插件</span></span><br><span class="line"><span class="comment"># HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">19. 代理，需要在环境变量中设置</span></span><br><span class="line"><span class="string">    from scrapy.contrib.downloadermiddleware.httpproxy import HttpProxyMiddleware</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    方式一：使用默认</span></span><br><span class="line"><span class="string">        os.environ</span></span><br><span class="line"><span class="string">        &#123;</span></span><br><span class="line"><span class="string">            http_proxy:http://root:woshiniba@192.168.11.11:9999/</span></span><br><span class="line"><span class="string">            https_proxy:http://192.168.11.11:9999/</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">    方式二：使用自定义下载中间件</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    def to_bytes(text, encoding=None, errors='strict'):</span></span><br><span class="line"><span class="string">        if isinstance(text, bytes):</span></span><br><span class="line"><span class="string">            return text</span></span><br><span class="line"><span class="string">        if not isinstance(text, six.string_types):</span></span><br><span class="line"><span class="string">            raise TypeError('to_bytes must receive a unicode, str or bytes '</span></span><br><span class="line"><span class="string">                            'object, got %s' % type(text).__name__)</span></span><br><span class="line"><span class="string">        if encoding is None:</span></span><br><span class="line"><span class="string">            encoding = 'utf-8'</span></span><br><span class="line"><span class="string">        return text.encode(encoding, errors)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    class ProxyMiddleware(object):</span></span><br><span class="line"><span class="string">        def process_request(self, request, spider):</span></span><br><span class="line"><span class="string">            PROXIES = [</span></span><br><span class="line"><span class="string">                &#123;'ip_port': '111.11.228.75:80', 'user_pass': ''&#125;,</span></span><br><span class="line"><span class="string">                &#123;'ip_port': '120.198.243.22:80', 'user_pass': ''&#125;,</span></span><br><span class="line"><span class="string">                &#123;'ip_port': '111.8.60.9:8123', 'user_pass': ''&#125;,</span></span><br><span class="line"><span class="string">                &#123;'ip_port': '101.71.27.120:80', 'user_pass': ''&#125;,</span></span><br><span class="line"><span class="string">                &#123;'ip_port': '122.96.59.104:80', 'user_pass': ''&#125;,</span></span><br><span class="line"><span class="string">                &#123;'ip_port': '122.224.249.122:8088', 'user_pass': ''&#125;,</span></span><br><span class="line"><span class="string">            ]</span></span><br><span class="line"><span class="string">            proxy = random.choice(PROXIES)</span></span><br><span class="line"><span class="string">            if proxy['user_pass'] is not None:</span></span><br><span class="line"><span class="string">                request.meta['proxy'] = to_bytes（"http://%s" % proxy['ip_port']）</span></span><br><span class="line"><span class="string">                encoded_user_pass = base64.encodestring(to_bytes(proxy['user_pass']))</span></span><br><span class="line"><span class="string">                request.headers['Proxy-Authorization'] = to_bytes('Basic ' + encoded_user_pass)</span></span><br><span class="line"><span class="string">                print "**************ProxyMiddleware have pass************" + proxy['ip_port']</span></span><br><span class="line"><span class="string">            else:</span></span><br><span class="line"><span class="string">                print "**************ProxyMiddleware no pass************" + proxy['ip_port']</span></span><br><span class="line"><span class="string">                request.meta['proxy'] = to_bytes("http://%s" % proxy['ip_port'])</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    DOWNLOADER_MIDDLEWARES = &#123;</span></span><br><span class="line"><span class="string">       'step8_king.middlewares.ProxyMiddleware': 500,</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"> </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">20. Https访问</span></span><br><span class="line"><span class="string">    Https访问时有两种情况：</span></span><br><span class="line"><span class="string">    1. 要爬取网站使用的可信任证书(默认支持)</span></span><br><span class="line"><span class="string">        DOWNLOADER_HTTPCLIENTFACTORY = "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory"</span></span><br><span class="line"><span class="string">        DOWNLOADER_CLIENTCONTEXTFACTORY = "scrapy.core.downloader.contextfactory.ScrapyClientContextFactory"</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    2. 要爬取网站使用的自定义证书</span></span><br><span class="line"><span class="string">        DOWNLOADER_HTTPCLIENTFACTORY = "scrapy.core.downloader.webclient.ScrapyHTTPClientFactory"</span></span><br><span class="line"><span class="string">        DOWNLOADER_CLIENTCONTEXTFACTORY = "step8_king.https.MySSLFactory"</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        # https.py</span></span><br><span class="line"><span class="string">        from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory</span></span><br><span class="line"><span class="string">        from twisted.internet.ssl import (optionsForClientTLS, CertificateOptions, PrivateCertificate)</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        class MySSLFactory(ScrapyClientContextFactory):</span></span><br><span class="line"><span class="string">            def getCertificateOptions(self):</span></span><br><span class="line"><span class="string">                from OpenSSL import crypto</span></span><br><span class="line"><span class="string">                v1 = crypto.load_privatekey(crypto.FILETYPE_PEM, open('/Users/wupeiqi/client.key.unsecure', mode='r').read())</span></span><br><span class="line"><span class="string">                v2 = crypto.load_certificate(crypto.FILETYPE_PEM, open('/Users/wupeiqi/client.pem', mode='r').read())</span></span><br><span class="line"><span class="string">                return CertificateOptions(</span></span><br><span class="line"><span class="string">                    privateKey=v1,  # pKey对象</span></span><br><span class="line"><span class="string">                    certificate=v2,  # X509对象</span></span><br><span class="line"><span class="string">                    verify=False,</span></span><br><span class="line"><span class="string">                    method=getattr(self, 'method', getattr(self, '_ssl_method', None))</span></span><br><span class="line"><span class="string">                )</span></span><br><span class="line"><span class="string">    其他：</span></span><br><span class="line"><span class="string">        相关类</span></span><br><span class="line"><span class="string">            scrapy.core.downloader.handlers.http.HttpDownloadHandler</span></span><br><span class="line"><span class="string">            scrapy.core.downloader.webclient.ScrapyHTTPClientFactory</span></span><br><span class="line"><span class="string">            scrapy.core.downloader.contextfactory.ScrapyClientContextFactory</span></span><br><span class="line"><span class="string">        相关配置</span></span><br><span class="line"><span class="string">            DOWNLOADER_HTTPCLIENTFACTORY</span></span><br><span class="line"><span class="string">            DOWNLOADER_CLIENTCONTEXTFACTORY</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">21. 爬虫中间件</span></span><br><span class="line"><span class="string">    class SpiderMiddleware(object):</span></span><br><span class="line"><span class="string">        def process_spider_input(self,response, spider):</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line"><span class="string">            下载完成，执行，然后交给parse处理</span></span><br><span class="line"><span class="string">            :param response: </span></span><br><span class="line"><span class="string">            :param spider: </span></span><br><span class="line"><span class="string">            :return: </span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line"><span class="string">            pass</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        def process_spider_output(self,response, result, spider):</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line"><span class="string">            spider处理完成，返回时调用</span></span><br><span class="line"><span class="string">            :param response:</span></span><br><span class="line"><span class="string">            :param result:</span></span><br><span class="line"><span class="string">            :param spider:</span></span><br><span class="line"><span class="string">            :return: 必须返回包含 Request 或 Item 对象的可迭代对象(iterable)</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line"><span class="string">            return result</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        def process_spider_exception(self,response, exception, spider):</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line"><span class="string">            异常调用</span></span><br><span class="line"><span class="string">            :param response:</span></span><br><span class="line"><span class="string">            :param exception:</span></span><br><span class="line"><span class="string">            :param spider:</span></span><br><span class="line"><span class="string">            :return: None,继续交给后续中间件处理异常；含 Response 或 Item 的可迭代对象(iterable)，交给调度器或pipeline</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line"><span class="string">            return None</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        def process_start_requests(self,start_requests, spider):</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line"><span class="string">            爬虫启动时调用</span></span><br><span class="line"><span class="string">            :param start_requests:</span></span><br><span class="line"><span class="string">            :param spider:</span></span><br><span class="line"><span class="string">            :return: 包含 Request 对象的可迭代对象</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line"><span class="string">            return start_requests</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    内置爬虫中间件：</span></span><br><span class="line"><span class="string">        'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.spidermiddleware.urllength.UrlLengthMiddleware': 800,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># from scrapy.contrib.spidermiddleware.referer import RefererMiddleware</span></span><br><span class="line"><span class="comment"># Enable or disable spider middlewares</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line">SPIDER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="comment"># 'step8_king.middlewares.SpiderMiddleware': 543,</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">22. 下载中间件</span></span><br><span class="line"><span class="string">    class DownMiddleware1(object):</span></span><br><span class="line"><span class="string">        def process_request(self, request, spider):</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line"><span class="string">            请求需要被下载时，经过所有下载器中间件的process_request调用</span></span><br><span class="line"><span class="string">            :param request:</span></span><br><span class="line"><span class="string">            :param spider:</span></span><br><span class="line"><span class="string">            :return:</span></span><br><span class="line"><span class="string">                None,继续后续中间件去下载；</span></span><br><span class="line"><span class="string">                Response对象，停止process_request的执行，开始执行process_response</span></span><br><span class="line"><span class="string">                Request对象，停止中间件的执行，将Request重新调度器</span></span><br><span class="line"><span class="string">                raise IgnoreRequest异常，停止process_request的执行，开始执行process_exception</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line"><span class="string">            pass</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        def process_response(self, request, response, spider):</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line"><span class="string">            spider处理完成，返回时调用</span></span><br><span class="line"><span class="string">            :param response:</span></span><br><span class="line"><span class="string">            :param result:</span></span><br><span class="line"><span class="string">            :param spider:</span></span><br><span class="line"><span class="string">            :return:</span></span><br><span class="line"><span class="string">                Response 对象：转交给其他中间件process_response</span></span><br><span class="line"><span class="string">                Request 对象：停止中间件，request会被重新调度下载</span></span><br><span class="line"><span class="string">                raise IgnoreRequest 异常：调用Request.errback</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line"><span class="string">            print('response1')</span></span><br><span class="line"><span class="string">            return response</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        def process_exception(self, request, exception, spider):</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line"><span class="string">            当下载处理器(download handler)或 process_request() (下载中间件)抛出异常</span></span><br><span class="line"><span class="string">            :param response:</span></span><br><span class="line"><span class="string">            :param exception:</span></span><br><span class="line"><span class="string">            :param spider:</span></span><br><span class="line"><span class="string">            :return:</span></span><br><span class="line"><span class="string">                None：继续交给后续中间件处理异常；</span></span><br><span class="line"><span class="string">                Response对象：停止后续process_exception方法</span></span><br><span class="line"><span class="string">                Request对象：停止中间件，request将会被重新调用下载</span></span><br><span class="line"><span class="string">            '''</span></span><br><span class="line"><span class="string">            return None</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    默认下载中间件</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">        'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.downloadermiddleware.httpauth.HttpAuthMiddleware': 300,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': 400,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': 500,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.downloadermiddleware.redirect.MetaRefreshMiddleware': 580,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware': 590,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware': 600,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware': 700,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 750,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.downloadermiddleware.chunked.ChunkedTransferMiddleware': 830,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.downloadermiddleware.stats.DownloaderStats': 850,</span></span><br><span class="line"><span class="string">        'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># from scrapy.contrib.downloadermiddleware.httpauth import HttpAuthMiddleware</span></span><br><span class="line"><span class="comment"># Enable or disable downloader middlewares</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line"><span class="comment"># DOWNLOADER_MIDDLEWARES = &#123;</span></span><br><span class="line"><span class="comment">#    'step8_king.middlewares.DownMiddleware1': 100,</span></span><br><span class="line"><span class="comment">#    'step8_king.middlewares.DownMiddleware2': 500,</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure>
<p>此文为转载<a href="https://www.cnblogs.com/wupeiqi/articles/6229292.html" target="_blank" rel="noopener">https://www.cnblogs.com/wupeiqi/articles/6229292.html</a></p>

                
                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2021/07/30/lepeng-python-Python-项目通用目录结构/" data-toggle="tooltip" data-placement="top" title="Python 项目通用目录结构.py">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2021/07/30/lepeng-python-spider-spider-手写/" data-toggle="tooltip" data-placement="top" title="spider - 手写">Next Post &rarr;</a>
                    </li>
                    
                </ul>

                <!-- tip start -->
                

                
                <div class="comment_notes">
                    <p>
                        study well and make progress every day; study well and progress every day; study hard and make progress every day.
                    </p>
                </div>
                
                <!-- tip end -->

                <!-- Music start-->
                
                <!-- Music end -->

                <!-- Sharing -->
                
                <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                <!--  css & js -->
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            
              <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#Scrapy-简介"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">Scrapy 简介</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#一、安装"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">一、安装</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#二、基本使用"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">二、基本使用</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#1-基本命令"><span class="toc-nav-number">3.1.</span> <span class="toc-nav-text">1. 基本命令</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-项目结构以及爬虫应用简介"><span class="toc-nav-number">3.2.</span> <span class="toc-nav-text">2.项目结构以及爬虫应用简介</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#3-小试牛刀"><span class="toc-nav-number">3.3.</span> <span class="toc-nav-text">3.小试牛刀</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#4-选择器"><span class="toc-nav-number">3.4.</span> <span class="toc-nav-text">4. 选择器</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#5-格式化处理-pipelines"><span class="toc-nav-number">3.5.</span> <span class="toc-nav-text">5. 格式化处理 pipelines</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#6-中间件"><span class="toc-nav-number">3.6.</span> <span class="toc-nav-text">6.中间件</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#爬虫中间件"><span class="toc-nav-number">3.6.1.</span> <span class="toc-nav-text">爬虫中间件</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#下载器中间件"><span class="toc-nav-number">3.6.2.</span> <span class="toc-nav-text">下载器中间件</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#7-自定制命令"><span class="toc-nav-number">3.7.</span> <span class="toc-nav-text">7. 自定制命令</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#8-自定义扩展"><span class="toc-nav-number">3.8.</span> <span class="toc-nav-text">8. 自定义扩展</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#9-避免重复访问"><span class="toc-nav-number">3.9.</span> <span class="toc-nav-text">9. 避免重复访问</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#10-其他"><span class="toc-nav-number">3.10.</span> <span class="toc-nav-text">10.其他</span></a></li></ol></li></ol>
            
          
          </div>
        </aside>
      
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#Python" title="Python">Python</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="https://blog.csdn.net/fenglepeng" target="_blank">Feng Lepeng&#39;s CSDN</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>


<style  type="text/css">
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                

                

                

                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy;
                    <a href="https://beian.miit.gov.cn/" target="_blank" rel="noopener">京ICP备18055501号-2</a>;
                    Feng Lepeng 2022
                    <br>
                    Powered by 
                    <a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">
                        <i>hexo-theme-snail</i>
                    </a> | 
                    <iframe name="star" style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0"
                        width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=flepeng&repo=hexo-theme-lp&type=star">
                    </iframe>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://flepeng.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->


<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        

        
            <script type="text/javascript" src="/js/mouse-click.js" content='[&quot;🌱&quot;,&quot;just do it&quot;,&quot;🍀&quot;]' color='[&quot;rgb(121,93,179)&quot; ,&quot;rgb(76,180,231)&quot; ,&quot;rgb(184,90,154)&quot;]'></script>
        

        <!-- background effects end -->
    

    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!--<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
</body>

</html>
