<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <script data-ad-client="ca-pub-2488174175014870" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script> <!-- google ad -->
    <meta name="google-site-verification" content="40lMg4eqLLbXoDcpN3h-cEnfmselbQ8tUzNvuC0IRIs" /><!-- google 站点认证 -->
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="description" content="A hexo theme">
    <meta name="keyword"  content="hexo, hexo-theme-lp">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          深度学习之 SSD(Single Shot MultiBox Detector) - Hexo-theme-lp
        
    </title>

    <link rel="canonical" href="https://flepeng.github.io/2021/07/06/lepeng-ml-深度学习之-SSD/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
        
<link rel="stylesheet" href="/css/dusign-light.css">

        
<link rel="stylesheet" href="/css/dusign-common-light.css">

        
<link rel="stylesheet" href="/css/font-awesome.css">

        
<link rel="stylesheet" href="/css/toc.css">

        <!-- background effects end -->
    
    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">

    <!-- photography -->
    
<link rel="stylesheet" href="/css/photography.css">


    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- background effects start -->
    
    <!-- background effects end -->

	<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url('/img/header_img/archive-bg.jpg')
                /*post*/
            
        
    }
    
    #signature{
        background-image: url('/img/signature/dusign.png');
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#深度学习" title="深度学习">深度学习</a>
                            
                        </div>
                        <h1>深度学习之 SSD(Single Shot MultiBox Detector)</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by Lepeng on
                            2021-07-06
                        </span>

                        
                            <div class="blank_box"></div>
                            <span class="meta">
                                Words <span class="post-count">6.3k</span> and
                                Reading Time <span class="post-count">25</span> Minutes
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
    <div class="waveWrapper">
        <div class="wave wave_before" style="background-image: url('/img/wave-light.png')"></div>
        <div class="wave wave_after" style="background-image: url('/img/wave-light.png')"></div>
    </div>
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Lepeng</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/categories/">Categories</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    
                    
                    
                    <li>
                        <a href="http://flepeng.com" target="_blank">chinese_blog</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <p>目标检测近年来已经取得了很重要的进展，主流的算法主要分为两个类型：</p>
<p>（1）two-stage方法，如R-CNN系算法，其主要思路是先通过启发式方法（selective search）或者CNN网络（RPN)产生一系列稀疏的候选框，然后对这些候选框进行分类与回归，two-stage方法的优势是准确度高；<br>（2）one-stage方法，如Yolo和SSD，其主要思路是均匀地在图片的不同位置进行密集抽样，抽样时可以采用不同尺度和长宽比，然后利用CNN提取特征后直接进行分类与回归，整个过程只需要一步，所以其优势是速度快，但是均匀的密集采样的一个重要缺点是训练比较困难，这主要是因为正样本与负样本（背景）极其不均衡（参见Focal Loss，<a href="https://arxiv.org/abs/1708.02002），导致模型准确度稍低。不同算法的性能如图1所示，可以看到两类方法在准确度和速度上的差异。" target="_blank" rel="noopener">https://arxiv.org/abs/1708.02002），导致模型准确度稍低。不同算法的性能如图1所示，可以看到两类方法在准确度和速度上的差异。</a></p>
<p><img src="/img/ml/1625565815-60ee4b31ca7623b350ec3398a08f83ab.jpeg" alt=""></p>
<center><b><font size ='2'>图1 不同检测算法的性能对比</font></b></center>

<p>本文讲解的是SSD算法，其英文全名是Single Shot MultiBox Detector，Single shot指明了SSD算法属于one-stage方法，MultiBox指明了SSD是多框预测。从图1也可以看到，SSD算法在准确度和速度（除了SSD512）上都比Yolo要好很多。图2给出了不同算法的基本框架图，对于Faster R-CNN，其先通过CNN得到候选框，然后再进行分类与回归，而Yolo与SSD可以一步到位完成检测。相比Yolo，SSD采用CNN来直接进行检测，而不是</p>
<p><img src="/img/ml/1625565815-95da1fd980af1081f4c63624358330f7.jpeg" alt=""></p>
<p>像Yolo那样在全连接层之后做检测。其实采用卷积直接做检测只是SSD相比Yolo的其中一个不同点，另外还有两个重要的改变，一是SSD提取了不同尺度的特征图来做检测，大尺度特征图（较靠前的特征图）可以用来检测小物体，而小尺度特征图（较靠后的特征图）用来检测大物体；二是SSD采用了不同尺度和长宽比的先验框（Prior boxes, Default boxes，在Faster R-CNN中叫做锚，Anchors）。Yolo算法缺点是难以检测小目标，而且定位不准，但是这几点重要改进使得SSD在一定程度上克服这些缺点。下面我们详细讲解SDD算法的原理，并最后给出如何用TensorFlow实现SSD算法。</p>
<p><img src="/img/ml/1625565815-95da1fd980af1081f4c63624358330f7.jpeg" alt=""></p>
<center><b><font size ='2'>图2 不同算法的基本框架图</font></b></center>

<h2 id="设计理念"><a href="#设计理念" class="headerlink" title="设计理念"></a>设计理念</h2><p>SSD 和 Yolo 一样都是采用一个CNN网络来进行检测，但是却采用了多尺度的特征图，其基本架构如图3所示。下面将SSD核心设计理念总结为以下三点：</p>
<p><img src="/img/ml/1625565815-55c4c187dfe6ee94f377718a00dcab62.jpeg" alt="图3 SSD基本框架"></p>
<h3 id="1-采用多尺度特征图用于检测"><a href="#1-采用多尺度特征图用于检测" class="headerlink" title="(1)采用多尺度特征图用于检测"></a>(1)采用多尺度特征图用于检测</h3><p>所谓多尺度采用大小不同的特征图，CNN网络一般前面的特征图比较大，后面会逐渐采用stride=2的卷积或者pool来降低特征图大小，这正如图3所示，一个比较大的特征图和一个比较小的特征图，它们都用来做检测。这样做的好处是比较大的特征图来用来检测相对较小的目标，而小的特征图负责检测大目标，如图4所示，8x8的特征图可以划分更多的单元，但是其每个单元的先验框尺度比较小。</p>
<p><img src="/img/ml/1625565815-4ed1e21023c9d8a74a809498c48e67c7.png" alt="图4 不同尺度的特征图"></p>
<h3 id="2-采用卷积进行检测"><a href="#2-采用卷积进行检测" class="headerlink" title="(2)采用卷积进行检测"></a>(2)采用卷积进行检测</h3><p>与Yolo最后采用全连接层不同，SSD直接采用卷积对不同的特征图来进行提取检测结果。对于形状为m*n*p,的特征图，只需要采用3*3*3的特征图，只需要采用。</p>
<h3 id="3-设置先验框"><a href="#3-设置先验框" class="headerlink" title="(3)设置先验框"></a>(3)设置先验框</h3><p>在Yolo中，每个单元预测多个边界框，但是其都是相对这个单元本身（正方块），但是真实目标的形状是多变的，Yolo需要在训练过程中自适应目标的形状。而SSD借鉴了Faster R-CNN中anchor的理念，每个单元设置尺度或者长宽比不同的先验框，预测的边界框（bounding boxes）是以这些先验框为基准的，在一定程度上减少训练难度。一般情况下，每个单元会设置多个先验框，其尺度和长宽比存在差异，如图5所示，可以看到每个单元使用了4个不同的先验框，图片中猫和狗分别采用最适合它们形状的先验框来进行训练，后面会详细讲解训练过程中的先验框匹配原则。</p>
<p><img src="/img/ml/1625565815-35ba07c2bc7a3091c622e3bb63ec36e2.jpeg" alt="图5 SSD的先验框"></p>
<p>SSD的检测值也与Yolo不太一样。对于每个单元的每个先验框，其都输出一套独立的检测值，对应一个边界框，主要分为两个部分。第一部分是各个类别的置信度或者评分，值得注意的是SSD将背景也当做了一个特殊的类别，如果检测目标共有C个类别，SSD其实需要预测C+1个置信度值，其中第一个置信度指的是不含目标或者属于背景的评分。后面当我们说C个个类别置信度时，请记住里面包含背景那个特殊的类别，即真实的检测类别只有C-1个。在预测过程中，置信度最高的那个类别就是边界框所属的类别，特别地，当第一个置信度值最高时，表示边界框中并不包含目标。第二部分就是边界框的location，包含4个值（cx,cy,w,h），分别表示边界框的中心坐标以及宽高。但是真实预测值其实只是边界框相对于先验框的转换值(paper里面说是offset，但是觉得transformation更合适，参见R-CNN，<a href="https://arxiv.org/abs/1311.2524）。先验框位置用" target="_blank" rel="noopener">https://arxiv.org/abs/1311.2524）。先验框位置用</a> $d=(d^{cx}, d^{cy}, d^{w},d^{h})$，表示，其对应边界框用 $b=(b^{cx}, b^{cy}, b^{w},b^{h})$ 表示，那么边界框的预测值l,其实是b相对于d的转换值：</p>
<p><img src="/img/ml/1625565815-b496ee0c58670eed6142f398bc0a65f1.png" alt=""></p>
<p>习惯上，我们称上面这个过程为边界框的编码（encode），预测时，你需要反向这个过程，即进行解码（decode），从预测值l中得到边界框的真实位置b:</p>
<p><img src="/img/ml/1625565815-e632561dd3552338e6bcb26b9b82fd02.png" alt=""></p>
<p>然而，在SSD的Caffe源码（<a href="https://github.com/weiliu89/caffe/tree/ssd）实现中还有trick，那就是设置variance超参数来调整检测值，通过bool参数`variance_encoded_in_target`来控制两种模式，当其为True时，表示variance被包含在预测值中，就是上面那种情况。但是如果是Fasle（大部分采用这种方式，训练更容易？），就需要手动设置超参数variance，用来对l的4个值进行放缩，此时边界框需要这样解码：" target="_blank" rel="noopener">https://github.com/weiliu89/caffe/tree/ssd）实现中还有trick，那就是设置variance超参数来调整检测值，通过bool参数`variance_encoded_in_target`来控制两种模式，当其为True时，表示variance被包含在预测值中，就是上面那种情况。但是如果是Fasle（大部分采用这种方式，训练更容易？），就需要手动设置超参数variance，用来对l的4个值进行放缩，此时边界框需要这样解码：</a></p>
<p><img src="/img/ml/1625565815-b6f47625b39d094f42b562fdba2e609e.jpeg" alt=""></p>
<p>综上所述，对于一个大小m*n的特征图，共有mn个单元，每个单元设置的先验框数目记为k,那么每个单元共需要（c+4）k个预测值，所有的单元共需要（c+4）kmn个预测值，由于SSD采用卷积做检测，所以就需要（c+4）k个卷积核完成这个特征图的检测过程。</p>
<h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>SSD采用VGG16作为基础模型，然后在VGG16的基础上新增了卷积层来获得更多的特征图以用于检测。SSD的网络结构如图6所示。上面是SSD模型，下面是Yolo模型，可以明显看到SSD利用了多尺度的特征图做检测。模型的输入图片大小是300*300（还可以是512*512，其与前者网络结构没有差别，只是最后新增一个卷积层，本文不再讨论）。</p>
<p><img src="/img/ml/1625565815-226bf50e172dceee0ba5188c874e5830.jpeg" alt=""></p>
<center><b><font size ='2'>图5 SSD网络结构</font></b></center>

<p>SSD 采用VGG16做基础模型，首先VGG16是在ILSVRC CLS-LOC数据集预训练。然后借鉴了DeepLab-LargeFOV，(链接<a href="https://export.arxiv.org/pdf/1606.00915)，分别将VGG16的全连接层fc6和fc7转换成3\*3卷积层conv6和1\*1卷积层conv7，同时将池化层pool5由原来的2\*2-s2变成3\*3-s1(猜想是不想reduce特征图大小),为了配合这种变化，采用了一种Atrous" target="_blank" rel="noopener">https://export.arxiv.org/pdf/1606.00915)，分别将VGG16的全连接层fc6和fc7转换成3\*3卷积层conv6和1\*1卷积层conv7，同时将池化层pool5由原来的2\*2-s2变成3\*3-s1(猜想是不想reduce特征图大小),为了配合这种变化，采用了一种Atrous</a> Algorithm，其实就是conv6采用扩展卷积或带孔卷积（Dilation Conv，<a href="https://arxiv.org/abs/1511.07122），其在不增加参数与模型复杂度的条件下指数级扩大卷积的视野，其使用扩张率(dilation" target="_blank" rel="noopener">https://arxiv.org/abs/1511.07122），其在不增加参数与模型复杂度的条件下指数级扩大卷积的视野，其使用扩张率(dilation</a> rate)参数，来表示扩张的大小，如下图6所示，(a)是普通的3*3卷积，其视野就是3*3,，(b)是扩张率为2，此时视野变成7*7，(c)扩张率为4时，视野扩大为15*15，但是视野的特征更稀疏了。Conv6采用3*3大小但dilation rate=6的扩展卷积。</p>
<p><img src="/img/ml/1625565815-b3b47c8da55774fc578cef0ae883d310.png" alt=""></p>
<center><b><font size ='2'>图6 扩展卷积</font></b></center>

<p>然后移除dropout层和fc8层，并新增一系列卷积层，在检测数据集上做finetuing。</p>
<p>其中VGG16中的Conv4_3层将作为用于检测的第一个特征图。conv4_3层特征图大小是38*38，但是该层比较靠前，其norm较大，所以在其后面增加了一个L2 Normalization层（参见ParseNet，<a href="https://arxiv.org/abs/1506.04579），以保证和后面的检测层差异不是很大，这个和Batch" target="_blank" rel="noopener">https://arxiv.org/abs/1506.04579），以保证和后面的检测层差异不是很大，这个和Batch</a> Normalization层不太一样，其仅仅是对每个像素点在channle维度做归一化，而Batch Normalization层是在[batch_size, width, height]三个维度上做归一化。归一化后一般设置一个可训练的放缩变量gamma，使用TF可以这样简单实现：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># l2norm (not bacth norm, spatial normalization)</span><br><span class="line">def l2norm(x, scale, trainable&#x3D;True, scope&#x3D;&quot;L2Normalization&quot;):</span><br><span class="line">    n_channels &#x3D; x.get_shape().as_list()[-1]</span><br><span class="line">    l2_norm &#x3D; tf.nn.l2_normalize(x, [3], epsilon&#x3D;1e-12)</span><br><span class="line">    with tf.variable_scope(scope):</span><br><span class="line">        gamma &#x3D; tf.get_variable(&quot;gamma&quot;, shape&#x3D;[n_channels, ], dtype&#x3D;tf.float32,</span><br><span class="line">                                initializer&#x3D;tf.constant_initializer(scale),</span><br><span class="line">                                trainable&#x3D;trainable)</span><br><span class="line">        return l2_norm * gamma</span><br></pre></td></tr></table></figure>
<p>从后面新增的卷积层中提取Conv7，Conv8_2，Conv9_2，Conv10_2，Conv11_2作为检测所用的特征图，加上Conv4_3层，共提取了6个特征图，其大小分别是(38,38),(19,19),(10,10),(5,5),(3,3,),(1,1),,但是不同特征图设置的先验框数目不同（同一个特征图上每个单元设置的先验框是相同的，这里的数目指的是一个单元的先验框数目）。先验框的设置，包括尺度（或者说大小）和长宽比两个方面。对于先验框的尺度，其遵守一个线性递增规则：随着特征图大小降低，先验框尺度线性增加：</p>
<p><img src="/img/ml/1625570894-6778d804eb2266d36263d20bd711b9c4.png" alt=""></p>
<p>其中m指的特征图个数，但却是5，因为第一层（Conv4_3层）是单独设置的，$S_k$是先验框大小相对于图片的比例，而$S_min$和$S_max$表示比例的最小值与最大值，paper里面取0.2和0.9。对于第一个特征图，其先验框的尺度比例一般设置为$S_min/2=0.1$,那么尺度为300*0.1=30。对于后面的特征图，先验框尺度按照上面公式线性增加，但是先将尺度比例先扩大100倍，此时增长步长为</p>
<p><img src="/img/ml/1625570894-1a1dac21a3c8675332aba55f6760d2bc.png" alt=""></p>
<p>，这样各个特征图的$S_k$为20,37,54,71,88,将这些比例除以100，然后再乘以图片大小，可以得到各个特征图的尺度为60,111,162,213,264，这种计算方式是参考SSD的Caffe源码。综上，可以得到各个特征图的先验框尺度30,60,111,162,213,264。对于长宽比，一般选取</p>
<p><img src="/img/ml/1625570894-dc530feb9c09d3f4fd0d49693daf9a32.png" alt=""></p>
<p>，对于特定的长宽比，按如下公式计算先验框的宽度与高度（后面的Sk均指的是先验框实际尺度，而不是尺度比例）：</p>
<p><img src="/img/ml/1625570894-bcce0c6d27d90dded668dcffd1e49163.png" alt=""></p>
<p>默认情况下，每个特征图会有一个 $a_r=1$ 且尺度为 $S_k$ 的先验框，除此之外，还会设置一个尺度为 ${s}’<em>k=\sqrt{S_kS</em>{k+1}}$</p>
<p>且 $a<em>r=1$ 的先验框，这样每个特征图都设置了两个长宽比为1但大小不同的正方形先验框。注意最后一个特征图需要参考一个虚拟 $S</em>{m+1} = 300+105/100=315$来计算。因此，每个特征图一共有6个先验框${1,2,3,1/2,1/3,{1}’}$，但是在实现时，Conv4_3，Conv10_2和Conv11_2层仅使用4个先验框，它们不使用长宽比为3，1/3的先验框。每个单元的先验框的中心点分布在各个单元的中心，即</p>
<p><img src="/img/ml/1625571468-7949ce28872c82db61a4c055d65f2535.png" alt=""></p>
<p>，其中 $|f_k|$为特征图的大小。</p>
<p>得到了特征图之后，需要对特征图进行卷积得到检测结果，图7给出了一个5*5大小的特征图的检测过程。其中Priorbox是得到先验框，前面已经介绍了生成规则。检测值包含两个部分：类别置信度和边界框位置，各采用一次3*3卷积来进行完成。令 $n_k$为该特征图所采用的先验框数目，那么类别置信度需要的卷积核数量为$n_k <em> c$，而边界框位置需要的卷积核数量为$n_k </em> 4$。由于每个先验框都会预测一个边界框，所以SSD300一共可以预测</p>
<p><img src="/img/ml/1625571468-11831f20a420c1304b5da540eeacfd70.png" alt=""></p>
<p>个边界框，这是一个相当庞大的数字，所以说SSD本质上是密集采样。</p>
<p><img src="/img/ml/1625571468-a106346067cee3d0a87660714ddc8492.jpeg" alt=""></p>
<center><b><font size ='2'>图7 基于卷积得到检测结果</font></b></center>

<h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>（1）先验框匹配</p>
<p>在训练过程中，首先要确定训练图片中的ground truth（真实目标）与哪个先验框来进行匹配，与之匹配的先验框所对应的边界框将负责预测它。在Yolo中，ground truth的中心落在哪个单元格，该单元格中与其IOU最大的边界框负责预测它。但是在SSD中却完全不一样，SSD的先验框与ground truth的匹配原则主要有两点。首先，对于图片中每个ground truth，找到与其IOU最大的先验框，该先验框与其匹配，这样，可以保证每个ground truth一定与某个先验框匹配。通常称与ground truth匹配的先验框为正样本，反之，若一个先验框没有与任何ground truth进行匹配，那么该先验框只能与背景匹配，就是负样本。一个图片中ground truth是非常少的， 而先验框却很多，如果仅按第一个原则匹配，很多先验框会是负样本，正负样本极其不平衡，所以需要第二个原则。第二个原则是：对于剩余的未匹配先验框，若某个ground truth的IOU大于某个阈值（一般是0.5），那么该先验框也与这个ground truth进行匹配。这意味着某个ground truth可能与多个先验框匹配，这是可以的。但是反过来却不可以，因为一个先验框只能匹配一个ground truth，如果多个ground truth与某个先验框IOU大于阈值，那么先验框只与IOU最大的那个先验框进行匹配。第二个原则一定在第一个原则之后进行，仔细考虑一下这种情况，如果某个ground truth所对应最大IOU小于阈值，并且所匹配的先验框却与另外一个ground truth的IOU大于阈值，那么该先验框应该匹配谁，答案应该是前者，首先要确保某个ground truth一定有一个先验框与之匹配。但是，这种情况我觉得基本上是不存在的。由于先验框很多，某个ground truth的最大IOU肯定大于阈值，所以可能只实施第二个原则既可以了，这里的TensorFlow（<a href="https://github.com/xiaohu2015/SSD-Tensorflow/blob/master/nets/ssd\_common.py）版本就是只实施了第二个原则，但是这里的Pytorch（https://github.com/amdegroot/ssd.pytorch/blob/master/layers/box\_utils.py）两个原则都实施了。图8为一个匹配示意图，其中绿色的GT是ground" target="_blank" rel="noopener">https://github.com/xiaohu2015/SSD-Tensorflow/blob/master/nets/ssd\_common.py）版本就是只实施了第二个原则，但是这里的Pytorch（https://github.com/amdegroot/ssd.pytorch/blob/master/layers/box\_utils.py）两个原则都实施了。图8为一个匹配示意图，其中绿色的GT是ground</a> truth，红色为先验框，FP表示负样本，TP表示正样本。</p>
<p>图8 先验框匹配示意图</p>
<p>尽管一个ground truth可以与多个先验框匹配，但是ground truth相对先验框还是太少了，所以负样本相对正样本会很多。为了保证正负样本尽量平衡，SSD采用了hard negative mining，就是对负样本进行抽样，抽样时按照置信度误差（预测背景的置信度越小，误差越大）进行降序排列，选取误差的较大的top-k作为训练的负样本，以保证正负样本比例接近1:3。</p>
<p>（2）损失函数</p>
<p>训练样本确定了，然后就是损失函数了。损失函数定义为位置误差（locatization loss， loc）与置信度误差（confidence loss, conf）的加权和：</p>
<p>其中N是先验框的正样本数量。这里</p>
<p>为一个指示参数，当</p>
<p>时表示第i个先验框与第j个ground truth匹配，并且ground truth的类别为p。c为类别置信度预测值。l为先验框的所对应边界框的位置预测值，而g</p>
<p>是ground truth的位置参数。对于位置误差，其采用Smooth L1 loss，定义如下：</p>
<p>由于的</p>
<p>存在，所以位置误差仅针对正样本进行计算。值得注意的是，要先对ground truth的g进行编码得到</p>
<p>，因为预测值l也是编码值，若设置<code>variance_encoded_in_target=True</code>，编码时要加上variance：</p>
<p>对于置信度误差，其采用softmax loss:</p>
<p>权重系数a通过交叉验证设置为1。</p>
<p>（3）数据扩增</p>
<p>采用数据扩增（Data Augmentation）可以提升SSD的性能，主要采用的技术有水平翻转（horizontal flip），随机裁剪加颜色扭曲（random crop &amp; color distortion），随机采集块域（Randomly sample a patch）（获取小目标训练样本），如下图所示：</p>
<p>图9 数据扩增方案</p>
<p>其它的训练细节如学习速率的选择详见论文，这里不再赘述。</p>
<p>4</p>
<p>预测过程</p>
<p>预测过程比较简单，对于每个预测框，首先根据类别置信度确定其类别（置信度最大者）与置信度值，并过滤掉属于背景的预测框。然后根据置信度阈值（如0.5）过滤掉阈值较低的预测框。对于留下的预测框进行解码，根据先验框得到其真实的位置参数（解码后一般还需要做clip，防止预测框位置超出图片）。解码之后，一般需要根据置信度进行降序排列，然后仅保留top-k（如400）个预测框。最后就是进行NMS算法，过滤掉那些重叠度较大的预测框。最后剩余的预测框就是检测结果了。</p>
<p>5</p>
<p>性能评估</p>
<p>首先整体看一下SSD在VOC2007，VOC2012及COCO数据集上的性能，如表1所示。相比之下，SSD512的性能会更好一些。加*的表示使用了image expansion data augmentation（通过zoom out来创造小的训练样本）技巧来提升SSD在小目标上的检测效果，所以性能会有所提升。</p>
<p>表1 SSD在不同数据集上的性能</p>
<p>SSD与其它检测算法的对比结果（在VOC2007数据集）如表2所示，基本可以看到，SSD与Faster R-CNN有同样的准确度，并且与Yolo具有同样较快地检测速度。</p>
<p>表2 SSD与其它检测算法的对比结果（在VOC2007数据集）</p>
<p>文章还对SSD的各个trick做了更为细致的分析，表3为不同的trick组合对SSD的性能影响，从表中可以得出如下结论：</p>
<ul>
<li>数据扩增技术很重要，对于mAP的提升很大；</li>
<li>使用不同长宽比的先验框可以得到更好的结果；</li>
</ul>
<p>表3 不同的trick组合对SSD的性能影响</p>
<p>同样的，采用多尺度的特征图用于检测也是至关重要的，这可以从表4中看出：</p>
<p>表4 多尺度特征图对SSD的影响</p>
<p>6</p>
<p>TensorFlow上的实现</p>
<p>SSD在很多框架上都有了开源的实现，这里基于balancap的TensorFlow版本（<a href="https://github.com/balancap/SSD-Tensorflow）来实现SSD的Inference过程。这里实现的是SSD300，与paper里面不同的是，这里采用" target="_blank" rel="noopener">https://github.com/balancap/SSD-Tensorflow）来实现SSD的Inference过程。这里实现的是SSD300，与paper里面不同的是，这里采用</a></p>
<p>。首先定义SSD的参数：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">self.ssd_params = SSDParams(img_shape=(300, 300),   # 输入图片大小</span><br><span class="line">                                num_classes=21,     # 类别数+背景</span><br><span class="line">                                no_annotation_label=<span class="number">21</span>,</span><br><span class="line">                                feat_layers=["block4", "block7", "block8", "block9", "block10", "block11"],   # 要进行检测的特征图name</span><br><span class="line">                                feat_shapes=[(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)],  # 特征图大小</span><br><span class="line">                                anchor_size_bounds=[0.15, 0.90],  # 特征图尺度范围</span><br><span class="line">                                anchor_sizes=[(<span class="number">21.</span>, <span class="number">45.</span>),</span><br><span class="line">                                              (<span class="number">45.</span>, <span class="number">99.</span>),</span><br><span class="line">                                              (<span class="number">99.</span>, <span class="number">153.</span>),</span><br><span class="line">                                              (<span class="number">153.</span>, <span class="number">207.</span>),</span><br><span class="line">                                              (<span class="number">207.</span>, <span class="number">261.</span>),</span><br><span class="line">                                              (261., 315.)],  # 不同特征图的先验框尺度（第一个值是s_k，第2个值是s_k+1）</span><br><span class="line">                                anchor_ratios=[[<span class="number">2</span>, <span class="number">.5</span>],</span><br><span class="line">                                               [<span class="number">2</span>, <span class="number">.5</span>, <span class="number">3</span>, <span class="number">1.</span> / <span class="number">3</span>],</span><br><span class="line">                                               [<span class="number">2</span>, <span class="number">.5</span>, <span class="number">3</span>, <span class="number">1.</span> / <span class="number">3</span>],</span><br><span class="line">                                               [<span class="number">2</span>, <span class="number">.5</span>, <span class="number">3</span>, <span class="number">1.</span> / <span class="number">3</span>],</span><br><span class="line">                                               [<span class="number">2</span>, <span class="number">.5</span>],</span><br><span class="line">                                               [2, .5]], # 特征图先验框所采用的长宽比（每个特征图都有2个正方形先验框）</span><br><span class="line">                                anchor_steps=[8, 16, 32, 64, 100, 300],  # 特征图的单元大小</span><br><span class="line">                                anchor_offset=0.5,                       # 偏移值，确定先验框中心</span><br><span class="line">                                normalizations=[20, -1, -1, -1, -1, -1],  # l2 norm</span><br><span class="line">                                prior_scaling=[0.1, 0.1, 0.2, 0.2]       # variance</span><br><span class="line">                                )</span><br></pre></td></tr></table></figure>
<p>然后构建整个网络，注意对于stride=2的conv不要使用TF自带的padding=”same”，而是手动pad，这是为了与Caffe一致：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">def _built_net(self):</span><br><span class="line">    <span class="string">""</span><span class="string">"Construct the SSD net"</span><span class="string">""</span></span><br><span class="line">    self.end_points = &#123;&#125;  # record the detection layers output</span><br><span class="line">    self._images = tf.placeholder(tf.float32, shape=[None, self.ssd_params.img_shape[<span class="number">0</span>],</span><br><span class="line">                                                    self.ssd_params.img_shape[<span class="number">1</span>], <span class="number">3</span>])</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"ssd_300_vgg"</span>):</span><br><span class="line">        # original vgg layers</span><br><span class="line">        # block 1</span><br><span class="line">        net = conv2d(self._images, <span class="number">64</span>, <span class="number">3</span>, scope=<span class="string">"conv1_1"</span>)</span><br><span class="line">        net = conv2d(net, <span class="number">64</span>, <span class="number">3</span>, scope=<span class="string">"conv1_2"</span>)</span><br><span class="line">        self.end_points[<span class="string">"block1"</span>] = net</span><br><span class="line">        net = max_pool2d(net, <span class="number">2</span>, scope=<span class="string">"pool1"</span>)</span><br><span class="line">        # block 2</span><br><span class="line">        net = conv2d(net, <span class="number">128</span>, <span class="number">3</span>, scope=<span class="string">"conv2_1"</span>)</span><br><span class="line">        net = conv2d(net, <span class="number">128</span>, <span class="number">3</span>, scope=<span class="string">"conv2_2"</span>)</span><br><span class="line">        self.end_points[<span class="string">"block2"</span>] = net</span><br><span class="line">        net = max_pool2d(net, <span class="number">2</span>, scope=<span class="string">"pool2"</span>)</span><br><span class="line">        # block 3</span><br><span class="line">        net = conv2d(net, <span class="number">256</span>, <span class="number">3</span>, scope=<span class="string">"conv3_1"</span>)</span><br><span class="line">        net = conv2d(net, <span class="number">256</span>, <span class="number">3</span>, scope=<span class="string">"conv3_2"</span>)</span><br><span class="line">        net = conv2d(net, <span class="number">256</span>, <span class="number">3</span>, scope=<span class="string">"conv3_3"</span>)</span><br><span class="line">        self.end_points[<span class="string">"block3"</span>] = net</span><br><span class="line">        net = max_pool2d(net, <span class="number">2</span>, scope=<span class="string">"pool3"</span>)</span><br><span class="line">        # block 4</span><br><span class="line">        net = conv2d(net, <span class="number">512</span>, <span class="number">3</span>, scope=<span class="string">"conv4_1"</span>)</span><br><span class="line">        net = conv2d(net, <span class="number">512</span>, <span class="number">3</span>, scope=<span class="string">"conv4_2"</span>)</span><br><span class="line">        net = conv2d(net, <span class="number">512</span>, <span class="number">3</span>, scope=<span class="string">"conv4_3"</span>)</span><br><span class="line">        self.end_points[<span class="string">"block4"</span>] = net</span><br><span class="line">        net = max_pool2d(net, <span class="number">2</span>, scope=<span class="string">"pool4"</span>)</span><br><span class="line">        # block 5</span><br><span class="line">        net = conv2d(net, <span class="number">512</span>, <span class="number">3</span>, scope=<span class="string">"conv5_1"</span>)</span><br><span class="line">        net = conv2d(net, <span class="number">512</span>, <span class="number">3</span>, scope=<span class="string">"conv5_2"</span>)</span><br><span class="line">        net = conv2d(net, <span class="number">512</span>, <span class="number">3</span>, scope=<span class="string">"conv5_3"</span>)</span><br><span class="line">        self.end_points[<span class="string">"block5"</span>] = net</span><br><span class="line">        print(net)</span><br><span class="line">        net = max_pool2d(net, <span class="number">3</span>, stride=<span class="number">1</span>, scope=<span class="string">"pool5"</span>)</span><br><span class="line">        print(net)</span><br><span class="line"></span><br><span class="line">        # additional SSD layers</span><br><span class="line">        # block 6: use dilate conv</span><br><span class="line">        net = conv2d(net, <span class="number">1024</span>, <span class="number">3</span>, dilation_rate=<span class="number">6</span>, scope=<span class="string">"conv6"</span>)</span><br><span class="line">        self.end_points[<span class="string">"block6"</span>] = net</span><br><span class="line">        #net = dropout(net, is_training=self.is_training)</span><br><span class="line">        # block 7</span><br><span class="line">        net = conv2d(net, <span class="number">1024</span>, <span class="number">1</span>, scope=<span class="string">"conv7"</span>)</span><br><span class="line">        self.end_points[<span class="string">"block7"</span>] = net</span><br><span class="line">        # block 8</span><br><span class="line">        net = conv2d(net, <span class="number">256</span>, <span class="number">1</span>, scope=<span class="string">"conv8_1x1"</span>)</span><br><span class="line">        net = conv2d(pad2d(net, <span class="number">1</span>), <span class="number">512</span>, <span class="number">3</span>, stride=<span class="number">2</span>, scope=<span class="string">"conv8_3x3"</span>,</span><br><span class="line">                     padding=<span class="string">"valid"</span>)</span><br><span class="line">        self.end_points[<span class="string">"block8"</span>] = net</span><br><span class="line">        # block 9</span><br><span class="line">        net = conv2d(net, <span class="number">128</span>, <span class="number">1</span>, scope=<span class="string">"conv9_1x1"</span>)</span><br><span class="line">        net = conv2d(pad2d(net, <span class="number">1</span>), <span class="number">256</span>, <span class="number">3</span>, stride=<span class="number">2</span>, scope=<span class="string">"conv9_3x3"</span>,</span><br><span class="line">                     padding=<span class="string">"valid"</span>)</span><br><span class="line">        self.end_points[<span class="string">"block9"</span>] = net</span><br><span class="line">        # block 10</span><br><span class="line">        net = conv2d(net, <span class="number">128</span>, <span class="number">1</span>, scope=<span class="string">"conv10_1x1"</span>)</span><br><span class="line">        net = conv2d(net, <span class="number">256</span>, <span class="number">3</span>, scope=<span class="string">"conv10_3x3"</span>, padding=<span class="string">"valid"</span>)</span><br><span class="line">        self.end_points[<span class="string">"block10"</span>] = net</span><br><span class="line">        # block 11</span><br><span class="line">        net = conv2d(net, <span class="number">128</span>, <span class="number">1</span>, scope=<span class="string">"conv11_1x1"</span>)</span><br><span class="line">        net = conv2d(net, <span class="number">256</span>, <span class="number">3</span>, scope=<span class="string">"conv11_3x3"</span>, padding=<span class="string">"valid"</span>)</span><br><span class="line">        self.end_points[<span class="string">"block11"</span>] = net</span><br><span class="line"></span><br><span class="line">        # class and location predictions</span><br><span class="line">        predictions = []</span><br><span class="line">        logits = []</span><br><span class="line">        locations = []</span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(self.ssd_params.feat_layers):</span><br><span class="line">            cls, loc = ssd_multibox_layer(self.end_points[layer], self.ssd_params.num_classes,</span><br><span class="line">                                          self.ssd_params.anchor_sizes[i],</span><br><span class="line">                                          self.ssd_params.anchor_ratios[i],</span><br><span class="line">                                          self.ssd_params.normalizations[i], scope=layer+<span class="string">"_box"</span>)</span><br><span class="line">            predictions.append(tf.nn.softmax(cls))</span><br><span class="line">            logits.append(cls)</span><br><span class="line">            locations.append(loc)</span><br><span class="line">        <span class="keyword">return</span> predictions, logits, locations</span><br></pre></td></tr></table></figure>
<p>对于特征图的检测，这里单独定义了一个组合层ssd_multibox_layer，其主要是对特征图进行两次卷积，分别得到类别置信度与边界框位置：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># multibox layer: get class and location predicitions from detection layer</span><br><span class="line">def ssd_multibox_layer(x, num_classes, sizes, ratios, normalization=<span class="number">-1</span>, scope=<span class="string">"multibox"</span>):</span><br><span class="line">    pre_shape = x.get_shape().as_list()[<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">    pre_shape = [<span class="number">-1</span>] + pre_shape</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">        # l2 norm</span><br><span class="line">        <span class="keyword">if</span> normalization &gt; <span class="number">0</span>:</span><br><span class="line">            x = l2norm(x, normalization)</span><br><span class="line">            print(x)</span><br><span class="line">        # numbers of anchors</span><br><span class="line">        n_anchors = len(sizes) + len(ratios)</span><br><span class="line">        # location predictions</span><br><span class="line">        loc_pred = conv2d(x, n_anchors*<span class="number">4</span>, <span class="number">3</span>, activation=None, scope=<span class="string">"conv_loc"</span>)</span><br><span class="line">        loc_pred = tf.reshape(loc_pred, pre_shape + [n_anchors, <span class="number">4</span>])</span><br><span class="line">        # class prediction</span><br><span class="line">        cls_pred = conv2d(x, n_anchors*num_classes, <span class="number">3</span>, activation=None, scope=<span class="string">"conv_cls"</span>)</span><br><span class="line">        cls_pred = tf.reshape(cls_pred, pre_shape + [n_anchors, num_classes])</span><br><span class="line">        <span class="keyword">return</span> cls_pred, loc_pred</span><br></pre></td></tr></table></figure>
<p>对于先验框，可以基于numpy生成，定义在ssd_anchors.py文件中，链接为<a href="https://github.com/xiaohu2015/DeepLearning\_tutorials/blob/master/ObjectDetections/SSD/ssd\_anchors.py。结合先验框与检测值，对边界框进行过滤与解码：" target="_blank" rel="noopener">https://github.com/xiaohu2015/DeepLearning\_tutorials/blob/master/ObjectDetections/SSD/ssd\_anchors.py。结合先验框与检测值，对边界框进行过滤与解码：</a></p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classes, scores, bboxes = self._bboxes_select(predictions, locations)</span><br></pre></td></tr></table></figure>
<p>这里将得到过滤得到的边界框，其中classes, scores, bboxes分别表示类别，置信度值以及边界框位置。</p>
<p>基于训练好的权重文件在这里下载<a href="https://pan.baidu.com/s/1snhuTsT，这里对SSD进行测试：" target="_blank" rel="noopener">https://pan.baidu.com/s/1snhuTsT，这里对SSD进行测试：</a></p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">ssd_net = SSD()</span><br><span class="line">classes, scores, bboxes = ssd_net.detections()</span><br><span class="line">images = ssd_net.images()</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"># Restore SSD model.</span><br><span class="line">ckpt_filename = <span class="string">'./ssd_checkpoints/ssd_vgg_300_weights.ckpt'</span></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.restore(sess, ckpt_filename)</span><br><span class="line"></span><br><span class="line">img = cv2.imread(<span class="string">'./demo/dog.jpg'</span>)</span><br><span class="line">img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line">img_prepocessed = preprocess_image(img)   # 预处理图片，主要是归一化和resize</span><br><span class="line">rclasses, rscores, rbboxes = sess.run([classes, scores, bboxes],</span><br><span class="line">                                      feed_dict=&#123;<span class="attr">images</span>: img_prepocessed&#125;)</span><br><span class="line">rclasses, rscores, rbboxes = process_bboxes(rclasses, rscores, rbboxes)  # 处理预测框，包括clip,sort,nms</span><br><span class="line"></span><br><span class="line">plt_bboxes(img, rclasses, rscores, rbboxes)  # 绘制检测结果</span><br></pre></td></tr></table></figure>
<p>详细的代码放在GitHub上了，链接为<a href="https://github.com/xiaohu2015/DeepLearning\_tutorials/tree/master/ObjectDetections/SSD，然后看一下一个自然图片的检测效果：" target="_blank" rel="noopener">https://github.com/xiaohu2015/DeepLearning\_tutorials/tree/master/ObjectDetections/SSD，然后看一下一个自然图片的检测效果：</a></p>
<p>如果你想实现SSD的train过程，你可以参考附录里面的Caffe,TensorFlow以及Pytorch实现。</p>
<p>小结</p>
<p>SSD在Yolo的基础上主要改进了三点：多尺度特征图，利用卷积进行检测，设置先验框。这使得SSD在准确度上比Yolo更好，而且对于小目标检测效果也相对好一点。由于很多实现细节都包含在源码里面，文中有描述不准或者错误的地方在所难免，欢迎交流指正。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li>SSD: Single Shot MultiBox Detector 链接：<a href="https://arxiv.org/pdf/1611.10012.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.10012.pdf</a></li>
<li>SSD Slide 链接：<a href="http://www.cs.unc.edu/~wliu/papers/ssd\_eccv2016\_slide.pdf" target="_blank" rel="noopener">http://www.cs.unc.edu/~wliu/papers/ssd\_eccv2016\_slide.pdf</a></li>
<li>SSD Caffe 链接：<a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">https://github.com/weiliu89/caffe/tree/ssd</a></li>
<li>SSD TensorFlow 链接：<a href="https://github.com/balancap/SSD-Tensorflow" target="_blank" rel="noopener">https://github.com/balancap/SSD-Tensorflow</a></li>
<li>SSD Pytorch 链接：<a href="https://github.com/amdegroot/ssd.pytorch" target="_blank" rel="noopener">https://github.com/amdegroot/ssd.pytorch</a></li>
<li>leonardoaraujosantos Artificial Inteligence online book 链接：<a href="https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/single-shot-detectors/ssd.html" target="_blank" rel="noopener">https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/single-shot-detectors/ssd.html</a></li>
</ol>

                
                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2021/07/06/lepeng-ml-深度学习之-YOLO-v1-v2-v3/" data-toggle="tooltip" data-placement="top" title="深度学习之 YOLO v1,v2,v3">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2021/07/06/lepeng-ml-深度学习参数之-batch-size/" data-toggle="tooltip" data-placement="top" title="深度学习参数之 batch size">Next Post &rarr;</a>
                    </li>
                    
                </ul>

                <!-- tip start -->
                

                
                <div class="comment_notes">
                    <p>
                        study well and make progress every day; study well and progress every day; study hard and make progress every day.
                    </p>
                </div>
                
                <!-- tip end -->

                <!-- Music start-->
                
                <!-- Music end -->

                <!-- Sharing -->
                
                <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                <!--  css & js -->
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      
        <aside id="sidebar">
          <div id="toc" class="toc-article">
          <strong class="toc-title">Contents</strong>
          
            
              <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#设计理念"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">设计理念</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#1-采用多尺度特征图用于检测"><span class="toc-nav-number">1.1.</span> <span class="toc-nav-text">(1)采用多尺度特征图用于检测</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#2-采用卷积进行检测"><span class="toc-nav-number">1.2.</span> <span class="toc-nav-text">(2)采用卷积进行检测</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#3-设置先验框"><span class="toc-nav-number">1.3.</span> <span class="toc-nav-text">(3)设置先验框</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#网络结构"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">网络结构</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#训练过程"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">训练过程</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#参考文献"><span class="toc-nav-number">4.</span> <span class="toc-nav-text">参考文献</span></a></li></ol>
            
          
          </div>
        </aside>
      
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#深度学习" title="深度学习">深度学习</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="https://blog.csdn.net/fenglepeng" target="_blank">Feng Lepeng&#39;s CSDN</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>


<style  type="text/css">
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                

                

                

                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy;
                    <a href="https://beian.miit.gov.cn/" target="_blank" rel="noopener">京ICP备18055501号-2</a>;
                    Feng Lepeng 2022
                    <br>
                    Powered by 
                    <a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">
                        <i>hexo-theme-snail</i>
                    </a> | 
                    <iframe name="star" style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0"
                        width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=flepeng&repo=hexo-theme-lp&type=star">
                    </iframe>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://flepeng.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->


<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        

        
            <script type="text/javascript" src="/js/mouse-click.js" content='[&quot;🌱&quot;,&quot;just do it&quot;,&quot;🍀&quot;]' color='[&quot;rgb(121,93,179)&quot; ,&quot;rgb(76,180,231)&quot; ,&quot;rgb(184,90,154)&quot;]'></script>
        

        <!-- background effects end -->
    

    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!--<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
</body>

</html>
