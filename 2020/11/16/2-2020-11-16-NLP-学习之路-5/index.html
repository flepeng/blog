<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <script data-ad-client="ca-pub-2488174175014870" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script> <!-- google ad -->
    <meta name="google-site-verification" content="40lMg4eqLLbXoDcpN3h-cEnfmselbQ8tUzNvuC0IRIs" /><!-- google 站点认证 -->
    <meta name="baidu-site-verification" content="093lY4ziMu" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="description" content="A hexo theme">
    <meta name="keyword"  content="hexo, hexo-theme-lp">
    <link rel="shortcut icon" href="/img/ironman-draw.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <!--<link href='http://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>-->
    <title>
        
          NLP-学习之路-5 - Hexo-theme-lp
        
    </title>

    <link rel="canonical" href="https://flepeng.github.io/2020/11/16/2-2020-11-16-NLP-学习之路-5/">

    <!-- Bootstrap Core CSS -->
    
<link rel="stylesheet" href="/css/bootstrap.min.css">


    <!-- Custom CSS --> 
    
        
<link rel="stylesheet" href="/css/dusign-light.css">

        
<link rel="stylesheet" href="/css/dusign-common-light.css">

        
<link rel="stylesheet" href="/css/font-awesome.css">

        
<link rel="stylesheet" href="/css/toc.css">

        <!-- background effects end -->
    
    
    <!-- Pygments Highlight CSS -->
    
<link rel="stylesheet" href="/css/highlight.css">


    
<link rel="stylesheet" href="/css/widget.css">


    
<link rel="stylesheet" href="/css/rocket.css">


    
<link rel="stylesheet" href="/css/signature.css">


    
<link rel="stylesheet" href="/css/fonts.googleapis.css">


    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">

    <!-- photography -->
    
<link rel="stylesheet" href="/css/photography.css">


    <!-- ga & ba script hoook -->
    <script></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- background effects start -->
    
    <!-- background effects end -->

	<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            
                background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url('/undefined')
                /*post*/
            
        
    }
    
    #signature{
        background-image: url('/img/signature/dusign.png');
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#NLP" title="NLP">NLP</a>
                            
                        </div>
                        <h1>NLP-学习之路-5</h1>
                        <h2 class="subheading"></h2>
                        <span class="meta">
                            Posted by Lepeng on
                            2020-11-16
                        </span>

                        
                            <div class="blank_box"></div>
                            <span class="meta">
                                Words <span class="post-count">2.7k</span> and
                                Reading Time <span class="post-count">10</span> Minutes
                            </span>
                            <div class="blank_box"></div>
                            <!-- 不蒜子统计 start -->
                            <span class="meta">
                                Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
                            </span>
                            <!-- 不蒜子统计 end -->
                        

                    </div>
                

                </div>
            </div>
        </div>      
    </div>

    
    <div class="waveWrapper">
        <div class="wave wave_before" style="background-image: url('/img/wave-light.png')"></div>
        <div class="wave wave_after" style="background-image: url('/img/wave-light.png')"></div>
    </div>
    
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Lepeng</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                    

                        
                        <li>
                            <a href="/categories/">Categories</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    
                    
                    
                    <li>
                        <a href="http://flepeng.com" target="_blank">chinese_blog</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <h2 id="Word-Representation"><a href="#Word-Representation" class="headerlink" title="Word Representation"></a>Word Representation</h2><h3 id="1-Naive-representation-One-hot"><a href="#1-Naive-representation-One-hot" class="headerlink" title="1. Naïve representation (One-hot):"></a>1. Naïve representation (One-hot):</h3><p>用one-hot向量来表示，这个向量的维度是 $R^{\mid V \mid}$。也就是词袋模型。</p>
<p>缺点：</p>
<ol>
<li><p>维度过大且sparse，大部分是0</p>
</li>
<li><p>互相垂直，用点积来计算向量距离会有问题，相同词汇的距离为1，不同词汇的距离永远为0</p>
</li>
<li><p>没有语义上的区别，比如说无法单纯从向量上体现2个词是相似的，这其实和第2点有关。</p>
</li>
</ol>
<h3 id="2-Count-based-representation"><a href="#2-Count-based-representation" class="headerlink" title="2. Count-based representation:"></a>2. Count-based representation:</h3><p><strong>理论基础：</strong></p>
<p><strong>Distributional semantics,  a word’s meaning is given by the words that frequently appear close-by.</strong></p>
<p>方法：</p>
<ol>
<li>考虑目标词汇的上下文，利用窗口来圈出目标词上下文，假设目标词的个数为 $\mid V \mid$。</li>
<li>假设所有窗口上下文里面出现过的不同的词数为 $\mid D \mid$，那么每个目标词可以表示为一个长度为 $\mid D \mid$的向量，向量的每个位置对应着上下文出现过的词，每个位置的值为该词在目标词所有的上下文出现的次数。</li>
</ol>
<p>缺点：</p>
<ol>
<li>每个feature都有同等的重要性，不符合真实情况</li>
</ol>
<h3 id="3-TF-IDF-全称：Term-Frequency-Inverse-Document-Frequency"><a href="#3-TF-IDF-全称：Term-Frequency-Inverse-Document-Frequency" class="headerlink" title="3. TF-IDF (全称：Term Frequency - Inverse Document Frequency)"></a>3. TF-IDF (全称：Term Frequency - Inverse Document Frequency)</h3><p>TF-IDF用来衡量一个词的重要性，用来做词的特征选择，也常用于挖掘文章中的关键词，它由两部分组成：TF和IDF。</p>
<p><strong>TF 词频：</strong> 指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。</p>
<p><strong>IDF 逆文档频率：</strong>log（语料库的文档总数/包含该词的文档数）</p>
<p><strong>重要性：</strong>TF * IDF</p>
<p>缺点：</p>
<ol>
<li>因为是词袋模型，所以没有考虑词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。</li>
<li>无法体现词在上下文的重要性。</li>
</ol>
<h3 id="4-Embedding-Learning"><a href="#4-Embedding-Learning" class="headerlink" title="4. Embedding Learning"></a>4. Embedding Learning</h3><p>Embedding Learning说白了就是要学习词的向量表达：embedding matrix。</p>
<p>其实上面提到的count-based的方法也产生了一个 维度为 $R^{\mid V \mid \times \mid D \mid}$ 的embedding matrix。这种embedding matrix也称为<strong>Distributional Representation</strong>，是2种主流 word representation 方法的一种。另外一种是<strong>Distributed Representation</strong>，也就是这一小节的重点。</p>
<p>2种representation的特点：</p>
<ol>
<li><strong>Distributional Representation</strong>：使用词语的上下文来表示其语义，Word2vec和基于计数的词向量表示都属于这种。</li>
<li><strong>Distributed Representation</strong>：每个词并非采用离散的表征方式，而是被表征为一个连续的，低维，稠密的向量。与之相反的是one-hot向量。</li>
</ol>
<h4 id="4-1-Embedding-One-hot"><a href="#4-1-Embedding-One-hot" class="headerlink" title="4.1. Embedding + One-hot"></a>4.1. Embedding + One-hot</h4><p>假设一个词库的embedding matrix为 $E$，维度为$R^{\mid V \mid \times \mid D \mid}$。再假设词库的one-hot向量为 $O$，维度为 $R^{\mid V \mid \times 1}$。</p>
<p>那么一个词的向量可以由 $O^T \times E$ 获得。</p>
<h4 id="4-2-神经网络语言模型NNLM（重要节点-1）"><a href="#4-2-神经网络语言模型NNLM（重要节点-1）" class="headerlink" title="4.2. 神经网络语言模型NNLM（重要节点 - 1）"></a>4.2. 神经网络语言模型NNLM（重要节点 - 1）</h4><p>这是第一个基于神经网络的语言模型，在Bengio大神在2003年发表的《A Neural Probabilistic Language Model》被提出。NNLM首次提出了<strong>word embedding</strong>的概念（虽然没有叫这个名字），从而奠定了包括word2vec在内后续研究word representation learning的基础。</p>
<p><strong>NNLM的提出是为了解决N元语法模型的不足：</strong></p>
<ol>
<li>它无法处理更长程的context（N&gt;3），因为长程的context出现的概率会趋向于0。</li>
<li>它没有考虑词与词之间内在的联系性，或者说无法建模出词之间的相似度。这是因为，N元语法本质上是将词当做一个个孤立的原子单元去处理的。这种处理方式对应到数学上的形式是一个个离散的one-hot向量。</li>
<li>0概率的问题。（解决这个问题有两种常用方法： 平滑法和回退法。）</li>
</ol>
<p><strong>NNLM：</strong></p>
<ol>
<li><p>NNLM的问题设定和N元语法的目标函数一样的：</p>
<script type="math/tex; mode=display">
f\left(w_{t}, w_{t-1}, \cdots, w_{t-n+1}\right)=P\left(w_{t} \mid \text { context }\right)=P\left(w_{t} \mid w_{t-1}, \cdots, w_{t-n+1}\right)</script></li>
<li><p>仔细观察能发现 $1 / P\left(w<em>{t} \mid w</em>{t-1}, \cdots, w_{t-n+1}\right)$ 其实就是单个词的困惑度perplexity。 P越大，交叉熵和困惑度都越小。</p>
</li>
</ol>
<ol>
<li>NNLM的训练集：一个单词序列 $w_1, …, w_T$ （其实就是一个语料库）， 其中的每个词 $w_t \in V$，$V$ 表示一个有限的大词汇表。在N元语法中，其实训练集也是一样的，只不过是它不需要显式地去训练，而是在测试集中把需要算概率的每个词返回到训练集中数数就好了。</li>
</ol>
<ol>
<li><p>NNLM需要满足的限制：</p>
<ol>
<li>$\sum<em>{i=1}^{\mid V \mid} f\left(i, w</em>{t-1}, \cdots, w_{t-n+1}\right)=1$</li>
<li>$f &gt; 0 $</li>
</ol>
<p>一个softmax其实就满足了。</p>
</li>
</ol>
<ol>
<li><p>模型结构如下图。</p>
<p><img src="https://i.loli.net/2020/11/17/c5ywiRJkmITzZ6b.png" alt="image-20201117170304904"></p>
<p>整个模型可以分为2部分来理解：</p>
<ol>
<li><p>首先是一个线性的Embedding层。它将输入的N−1个one-hot词向量，通过一个共享的 $D \times V$ 的矩阵 $C$，映射为 $N−1$ 个distributed词向量。其中，$C$矩阵就是要学习的distributed embedding matrix。$V$是词典的大小，$D$是Embedding向量的维度（一个先验参数）。</p>
</li>
<li><p>其次是个简单的前向反馈神经网络 $g$，对词典中的word在输入context下的条件概率做出预估：</p>
<script type="math/tex; mode=display">
g\left(w_{t}, C(w_{t-1}), \cdots, C(w_{t-n+1})\right) = f\left(w_{t}, w_{t-1}, \cdots, w_{t-n+1}\right)=P\left(w_{t} \mid \text { context }\right)</script><p>其中，$C(w_{i})$表示词的embedding向量表示。也就是说，$g$ 的输入是 N-1 个embedding向量，$\text{input size} = (N-1) \times D$。假设隐含层有 $H$ 个单元，输入层到隐含层之间的权重矩阵的维度为 $H \times (N-1) \times D$，每个映射到隐含层的向量随后被tanh函数激活。隐含层到softmax层的矩阵维度是$V \times H$。</p>
</li>
</ol>
</li>
</ol>
<ol>
<li>损失函数是一个正则化的交叉熵：<script type="math/tex; mode=display">
L(\theta)=\frac{1}{T} \sum_{t} \log f\left(w_{t}, w_{t-1}, \ldots, w_{t-n+1}\right)+R(\theta)</script>其中，模型的参数 $\theta$ 包括了Embedding层矩阵$C$的元素，和前向反馈神经网络模型$g$里的权重。这是一个巨大的参数空间。不过，在用SGD学习更新模型的参数时，<strong>并不是所有的参数都需要调整（例如未在输入的context中出现的词对应的词向量）</strong>。<strong>计算的瓶颈主要是在softmax层的归一化函数上（需要对词典中所有的word计算一遍条件概率）</strong>。</li>
</ol>
<ol>
<li>仔细观察这个模型就会发现，它其实在同时解决两个问题：<strong>一个是统计语言模型里关注的条件概率 $p(w_ t \mid context)$ 的计算；一个是向量空间模型里关注的词向量的表达</strong>。而这两个问题本质上并不独立。通过引入连续的词向量和平滑的概率模型，我们就可以在一个连续空间里对序列概率进行建模，从而从根本上<strong>缓解数据稀疏性和维度灾难的问题</strong>。</li>
</ol>
<h4 id="4-3-Word2Vec（重要节点-2）"><a href="#4-3-Word2Vec（重要节点-2）" class="headerlink" title="4.3. Word2Vec（重要节点 - 2）"></a>4.3. Word2Vec（重要节点 - 2）</h4><p>首先先来看看NNLM存在的几个问题：</p>
<ol>
<li>同N元语法一样，NNLM模型只能处理定长的序列。Bengio将模型能够一次处理的序列长度N提高到了5，虽然相比bigram和trigram已经是很大的提升，但依然缺少灵活性。</li>
<li>NNLM的训练太慢了。</li>
</ol>
<p>4.2小节中我们提到了，NNLM模型可以分为2部分来理解。Mikolov意识到原始的NNLM模型计算的瓶颈主要是在softmax层的归一化函数上，也就是说瓶颈在模型的第二部分。如果我们只是想得到word的连续特征向量的话，是不是可以对第二步里的神经网络模型进行简化呢？这就是Word2Vec的出发点了。</p>
<p><strong>Word2Vec由2个模型组成：CBoW （Continuous Bag-of-Words Model）和 Skip-gram Model：</strong></p>
<p>其中CBOW如下图左部分所示，使用围绕目标单词的其他单词（语境）作为输入，在映射层做加权处理后输出目标单词。与CBOW根据语境预测目标单词不同，Skip-gram根据当前单词预测语境，如下图右部分所示。假如我们有一个句子“There is an apple on the table”作为训练数据，CBOW的输入为（is,an,on,the），输出为apple。而Skip-gram的输入为apple，输出为（is,an,on,the）。CBOW适合于数据集较小的情况，而Skip-Gram在大型语料中表现更好。</p>
<p><img src="https://i.loli.net/2020/11/19/3BoslpnR6PXZgzQ.png" alt="image-20201120003658435" style="zoom: 67%;" /></p>
<p><strong>首先来看CBoW ：</strong></p>
<p><img src="https://i.loli.net/2020/11/19/s9fTxOVy1dGWKEg.jpg" alt="img"></p>
<p>CBoW 对NNLM模型做了如下改造和简化（注意网上的这张图的notation和笔记中NNLM用到的notation不一样，对号入座即可）：</p>
<ol>
<li>移除前向反馈神经网络中非线性的hidden layer，直接将中间层的Embedding layer与输出层的softmax layer连接；</li>
<li>忽略上下文环境的序列信息：输入的所有词向量均汇总到同一个Embedding layer；也就是说不再为每个输入单词维护一个embedding向量了，只维护一个：所有输入单词的embedding向量会被平均。</li>
<li>将Future words纳入上下文环境。</li>
</ol>
<p><strong>接下来是Skip-gram Model：</strong></p>
<p><img src="https://i.loli.net/2020/11/19/5zXL7hZJInpd3r8.png" alt="img"></p>
<p>要点：</p>
<ol>
<li><p>训练数据是成对出现的，如下图所示：</p>
<p><img src="https://i.loli.net/2020/11/19/U4CW2qctjOln3Ma.png" alt="@skip-gram的训练样本" style="zoom:50%;" /></p>
</li>
<li><p>假设训练集为：一个单词序列 $w_1, …, w_T$， 其中的每个词 $w_t \in V$，$V$ 表示一个有限的大词汇表。那么Skip-gram Model输出层的维度是 $V \times V$。</p>
</li>
<li><p>目标函数是最小化交叉熵之和：</p>
<script type="math/tex; mode=display">
\frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p\left(w_{t+j} \mid w_{t}\right)</script><p>其中 $c$ 表示我们从中心词的一侧（左边或右边）选取词的数量，也叫做skip_window的参数。显然 $c$越大，每个词的训练数据对就越多。</p>
</li>
</ol>
<h4 id="4-3-Word2Vec的优化（重要节点-3）"><a href="#4-3-Word2Vec的优化（重要节点-3）" class="headerlink" title="4.3. Word2Vec的优化（重要节点 - 3）"></a>4.3. Word2Vec的优化（重要节点 - 3）</h4><p>然而，直接对词典里的V个词计算相似度并归一化，显然是一件极其耗时的impossible mission。为此，Mikolov引入了两种优化算法：<strong>Hierarchical Softmax 和 Negative Sampling</strong>。 （待补充）</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol>
<li><p>Bengio, Y., Ducharme, R., Vincent, P., &amp; Janvin, C. (2003). A neural probabilistic language model. The Journal of Machine Learning Research, 3, 1137–1155.</p>
<p><a href="https://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener">https://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf</a></p>
</li>
<li><p>Mikolov T , Chen K , Corrado G , et al. Efficient Estimation of Word Representations in Vector Space[J]. Computer Science, 2013. </p>
<p><a href="https://kopernio.com/viewer?doi=arXiv:1301.3781&amp;route=6" target="_blank" rel="noopener">https://kopernio.com/viewer?doi=arXiv:1301.3781&amp;route=6</a></p>
</li>
<li><p>Mikolov T , Sutskever I , Chen K , et al. Distributed Representations of Words and Phrases and their Compositionality[J]. Advances in Neural Information Processing Systems, 2013, 26:3111-3119.</p>
<p><a href="https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf</a></p>
</li>
<li><p><a href="https://www.cnblogs.com/guoyaohua/p/9240336.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/9240336.html</a></p>
</li>
<li><p>秦曾昌 - 自然语言处理算法精讲 - Chapter 5</p>
</li>
</ol>

                
                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2020/11/21/2-2020-11-21-避开Jekyll不支持的Latx-Math语法/" data-toggle="tooltip" data-placement="top" title="避开Jekyll不支持的Latex Math语法">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2020/11/12/2-2020-11-12-NLP-学习之路-4/" data-toggle="tooltip" data-placement="top" title="NLP-学习之路-4">Next Post &rarr;</a>
                    </li>
                    
                </ul>

                <!-- tip start -->
                

                
                <div class="comment_notes">
                    <p>
                        study well and make progress every day; study well and progress every day; study hard and make progress every day.
                    </p>
                </div>
                
                <!-- tip end -->

                <!-- Music start-->
                
                <!-- Music end -->

                <!-- Sharing -->
                
                <div class="social-share"  data-wechat-qrcode-helper="" align="center"></div>
                <!--  css & js -->
                <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css">
                <script src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>
                
                <!-- Sharing -->

                <!-- gitment start -->
                
                <!-- gitment end -->

                <!-- 来必力City版安装代码 -->
                
                <!-- City版安装代码已完成 -->

                <!-- disqus comment start -->
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#NLP" title="NLP">NLP</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <hr>
                <h5>FRIENDS</h5>
                <ul class="list-inline">

                    
                        <li><a href="https://blog.csdn.net/fenglepeng" target="_blank">Feng Lepeng&#39;s CSDN</a></li>
                    
                </ul>
                
            </div>
        </div>
    </div>
</article>




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>


<style  type="text/css">
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                

                

                

                

                

                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Feng Lepeng 2021 
                    <br>
                    Powered by 
                    <a href="https://github.com/dusign/hexo-theme-snail" target="_blank" rel="noopener">
                        <i>hexo-theme-snail</i>
                    </a> | 
                    <iframe name="star" style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0"
                        width="100px" height="20px"
                        src="https://ghbtns.com/github-btn.html?user=flepeng&repo=hexo-theme-lp&type=star">
                    </iframe>
                </p>
            </div>
        </div>
    </div>

</footer>

<!-- jQuery -->

<script src="/js/jquery.min.js"></script>


<!-- Bootstrap Core JavaScript -->

<script src="/js/bootstrap.min.js"></script>


<!-- Custom Theme JavaScript -->

<script src="/js/hux-blog.min.js"></script>


<!-- Search -->

<script src="/js/search.js"></script>


<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://flepeng.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->


<!-- Search -->

    <script type="text/javascript">      
        var search_path = "search.xml";
        if (search_path.length == 0) {
            search_path = "search.xml";
        }
    var path = "/" + search_path;
    searchFunc(path, 'local-search-input', 'local-search-result');
    </script>


<!-- busuanzi -->
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>

    
        <!-- background effects line -->
        

        
            <script type="text/javascript" src="/js/mouse-click.js" content='[&quot;🌱&quot;,&quot;just do it&quot;,&quot;🍀&quot;]' color='[&quot;rgb(121,93,179)&quot; ,&quot;rgb(76,180,231)&quot; ,&quot;rgb(184,90,154)&quot;]'></script>
        

        <!-- background effects end -->
    

    <!--<script size="50" alpha='0.3' zIndex="-999" src="/js/ribbonStatic.js"></script>-->
    
        <script src="/js/ribbonDynamic.js"></script>
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!--<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
</body>

</html>
